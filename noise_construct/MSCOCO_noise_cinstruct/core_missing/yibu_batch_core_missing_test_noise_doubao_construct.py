import base64
import os
import sys
import time
import signal
import json
import re
import asyncio
import aiofiles
from openai import AsyncOpenAI
from asyncio import Lock, Semaphore
from concurrent.futures import ThreadPoolExecutor

# ==================== é…ç½®å‚æ•° ====================
client = AsyncOpenAI(
    api_key="3d866616-54c8-4222-bb96-d5b6e208fbb7",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
)

# æµ‹è¯•æ¨¡å¼é…ç½®
TEST_MODE = False  # è®¾ç½®ä¸ºTrueå¯ç”¨æµ‹è¯•æ¨¡å¼
if TEST_MODE:
    # æµ‹è¯•æ¨¡å¼å‚æ•°
    MAX_CONCURRENT_REQUESTS = 2  # å¹¶å‘æ•°æ”¹ä¸º2
    SAVE_INTERVAL = 10  # æ¯10å¼ å›¾ç‰‡ä¿å­˜ä¸€ä¸ªæ–‡ä»¶ï¼ˆæµ‹è¯•ç”¨ï¼‰
    MAX_TEST_IMAGES = 10  # åªæµ‹è¯•å‰10å¼ å›¾ç‰‡
else:
    # ç”Ÿäº§æ¨¡å¼å‚æ•°
    MAX_CONCURRENT_REQUESTS = 20  # çœŸæ­£çš„å¹¶å‘æ•°
    SAVE_INTERVAL = 1000  # æ¯1000å¼ å›¾ç‰‡ä¿å­˜ä¸€ä¸ªæ–‡ä»¶
    MAX_TEST_IMAGES = None  # ä¸é™åˆ¶å›¾ç‰‡æ•°é‡

# å…¶ä»–é…ç½®
CHECKPOINT_INTERVAL = 50

# æ•°æ®é›†é…ç½®
DATASET_TYPE = 'coco'
IMAGE_DIR = '/home/zbm/xjd/NPC-master/dataset/core_missing_Error_noise_coco/images'
TEST_IDS_PATH = '/home/zbm/xjd/NPC-master/dataset/core_missing_Error_noise_coco/annotations/scan_split/test_ids.txt'
OUTPUT_DIR = '/home/zbm/xjd/NPC-master/MSCOCO_noise_cinstruct/core_missing/test_testid'

# æ–‡ä»¶é…ç½®
LOG_FILE = os.path.join(OUTPUT_DIR, 'processing_test.log')
CHECKPOINT_FILE = os.path.join(OUTPUT_DIR, 'checkpoint_test.json')
PID_FILE = os.path.join(OUTPUT_DIR, 'processing_test.pid')

# å¤„ç†å‚æ•°
MAX_API_RETRIES = 3
MAX_REGENERATION_ATTEMPTS = 5
API_TIMEOUT = 120.0

# å…¨å±€çº¿ç¨‹æ± ç”¨äºæ–‡ä»¶I/O
io_executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="FileIO")

# ==================== å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿ ====================
class AsyncLogger:
    """å¼‚æ­¥åŒé‡è¾“å‡ºï¼šæ§åˆ¶å°+æ—¥å¿—æ–‡ä»¶"""
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log_file = log_file
        self.lock = Lock()
        
    async def write(self, message):
        async with self.lock:
            self.terminal.write(message)
            self.terminal.flush()
            async with aiofiles.open(self.log_file, 'a', encoding='utf-8') as f:
                await f.write(message)
            
    def flush(self):
        self.terminal.flush()

    async def close(self):
        pass

# ==================== å…¨å±€é”å®šä¹‰ ====================
checkpoint_lock = None
failed_images_lock = None
logger = None

# ==================== ä¿¡å·å¤„ç† ====================
def signal_handler(signum, frame):
    print(f"\næ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨é€€å‡º...")
    sys.exit(0)

# ==================== æ•°æ®åŠ è½½ ====================
async def load_data():
    """åŠ è½½æµ‹è¯•é›†å›¾ç‰‡æ•°æ® - å¤„ç†é‡å¤ID"""
    print(f"æ­£åœ¨åŠ è½½æµ‹è¯•é›†IDæ–‡ä»¶: {TEST_IDS_PATH}")
    
    # é¦–å…ˆè¯»å–åŸå§‹æ–‡ä»¶å†…å®¹
    async with aiofiles.open(TEST_IDS_PATH, 'r', encoding='utf-8') as f:
        content = await f.read()
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºæ–‡ä»¶å†…å®¹
        print(f"åŸå§‹æ–‡ä»¶å†…å®¹å‰100å­—ç¬¦: {content[:100]}")
        print(f"æ–‡ä»¶æ€»å­—ç¬¦æ•°: {len(content)}")
        
        lines = content.strip().split('\n')
        print(f"åŸå§‹è¡Œæ•°: {len(lines)}")
        print(f"å‰10è¡Œ: {lines[:10]}")
    
    # æå–æ‰€æœ‰IDï¼ˆåŒ…å«é‡å¤ï¼‰
    all_ids = []
    for line in lines:
        line = line.strip()
        if line and line.isdigit():
            all_ids.append(int(line))
    
    print(f"\nğŸ“Š åŸå§‹IDæ•°é‡ï¼ˆåŒ…å«é‡å¤ï¼‰: {len(all_ids)}")
    
    # éªŒè¯æ–‡ä»¶æ ¼å¼
    print(f"\nğŸ” éªŒè¯test_ids.txtæ ¼å¼:")
    print(f"  æ€»è¡Œæ•°: {len(all_ids)}")
    
    if len(all_ids) % 5 == 0:
        print(f"  âœ… æ ¼å¼æ­£ç¡®: è¡Œæ•°æ˜¯5çš„å€æ•°")
    else:
        print(f"  âš  è­¦å‘Š: è¡Œæ•° {len(all_ids)} ä¸æ˜¯5çš„å€æ•°")
    
    # æ£€æŸ¥å‰å‡ ç»„
    print(f"\n  å‰3ç»„IDéªŒè¯:")
    for i in range(0, min(15, len(all_ids)), 5):
        group = all_ids[i:i+5]
        if len(group) == 5 and len(set(group)) == 1:
            print(f"    ç»„ {i//5 + 1}: ID {group[0]} é‡å¤5æ¬¡ - âœ… æ­£ç¡®")
        else:
            print(f"    ç»„ {i//5 + 1}: {group} - âŒ é”™è¯¯")
    
    # âœ… å…³é”®ä¿®æ”¹ï¼šæŒ‰ç…§test_ids.txtçš„æ ¼å¼ï¼Œæ¯5ä¸ªé‡å¤IDå¯¹åº”ä¸€å¼ å›¾ç‰‡
    unique_ids = []
    processed_ids_info = []
    
    # æ¯5ä¸ªIDä¸ºä¸€ç»„ï¼Œæå–ç¬¬ä¸€ä¸ªï¼ˆå› ä¸ºæ˜¯é‡å¤çš„ï¼‰
    for i in range(0, len(all_ids), 5):
        if i < len(all_ids):
            current_id = all_ids[i]
            unique_ids.append(current_id)
            processed_ids_info.append({
                'original_index': i,
                'id': current_id,
                'group_size': 5
            })
    
    print(f"\nâœ… å¤„ç†åå”¯ä¸€IDæ•°é‡ï¼ˆæŒ‰5ä¸ªä¸€ç»„ï¼‰: {len(unique_ids)}")
    print(f"ğŸ“Š é‡å¤ç»Ÿè®¡:")
    print(f"  æ€»å…± {len(all_ids)} è¡Œï¼Œæ¯5è¡Œå¯¹åº”ä¸€ä¸ªIDçš„5ä¸ªæè¿°")
    print(f"  æå–äº† {len(unique_ids)} ä¸ªå”¯ä¸€ID")
    
    # éªŒè¯IDåˆ†å¸ƒ
    print(f"\nğŸ” IDåˆ†å¸ƒéªŒè¯:")
    from collections import Counter
    id_counts = Counter(all_ids)
    for i in range(min(5, len(unique_ids))):
        id = unique_ids[i]
        count = id_counts[id]
        expected_count = 5  # æœŸæœ›æ¯ä¸ªIDé‡å¤5æ¬¡
        if count == expected_count:
            print(f"  ID {id}: å‡ºç° {count} æ¬¡ - âœ… ç¬¦åˆæœŸæœ›")
        else:
            print(f"  ID {id}: å‡ºç° {count} æ¬¡ - âš  æœŸæœ›æ˜¯{expected_count}æ¬¡")
    
    # âœ… ç”Ÿæˆå›¾ç‰‡åï¼šç›´æ¥ä½¿ç”¨æå–çš„ID
    image_names_to_process = []
    valid_ids = []
    
    print(f"\nğŸ“ æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨:")
    for idx, id in enumerate(unique_ids):
        # COCOå›¾ç‰‡æ ¼å¼ï¼šCOCO_2014_000000130524.jpg
        image_name = f'COCO_2014_{str(id).rjust(12, "0")}.jpg'
        image_path = os.path.join(IMAGE_DIR, image_name)
        
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if os.path.exists(image_path):
            image_names_to_process.append(image_name)
            valid_ids.append(id)
            if idx < 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  {idx+1}. {image_name} - âœ… å­˜åœ¨")
        else:
            print(f"  âŒ {image_name} - æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ID {id}")
            # ä½¿ç”¨å¤‡ç”¨æ£€æŸ¥æ–¹å¼
            # å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ–‡ä»¶åæ ¼å¼
            alt_names = [
                f'COCO_train2014_{str(id).rjust(12, "0")}.jpg',
                f'{id}.jpg',
                f'COCO_2014_{id}.jpg'
            ]
            found_alt = False
            for alt_name in alt_names:
                alt_path = os.path.join(IMAGE_DIR, alt_name)
                if os.path.exists(alt_path):
                    image_names_to_process.append(alt_name)
                    valid_ids.append(id)
                    print(f"  â¡ æ‰¾åˆ°å¤‡ç”¨æ–‡ä»¶å: {alt_name}")
                    found_alt = True
                    break
    
    if not image_names_to_process:
        print(f"\nâŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶!")
        print(f"è¯·æ£€æŸ¥å›¾ç‰‡ç›®å½•: {IMAGE_DIR}")
        print(f"å›¾ç‰‡åº”è¯¥ç±»ä¼¼: COCO_2014_000000130524.jpg")
        return []
    
    print(f"\nğŸ“Š å›¾ç‰‡æ–‡ä»¶éªŒè¯ç»“æœ:")
    print(f"  æ€»å…±æ‰¾åˆ° {len(image_names_to_process)} ä¸ªæœ‰æ•ˆå›¾ç‰‡æ–‡ä»¶")
    print(f"  å‰5ä¸ªå›¾ç‰‡:")
    for i in range(min(5, len(image_names_to_process))):
        img_path = os.path.join(IMAGE_DIR, image_names_to_process[i])
        print(f"  {i+1}. {image_names_to_process[i]}")
        print(f"     å®Œæ•´è·¯å¾„: {img_path}")
        print(f"     æ˜¯å¦å­˜åœ¨: {'âœ… æ˜¯' if os.path.exists(img_path) else 'âŒ å¦'}")
    
    # æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰Nå¼ å›¾ç‰‡
    if TEST_MODE and MAX_TEST_IMAGES:
        if len(image_names_to_process) > MAX_TEST_IMAGES:
            print(f"\nğŸ”¬ æµ‹è¯•æ¨¡å¼: åªå¤„ç†å‰ {MAX_TEST_IMAGES} å¼ å›¾ç‰‡")
            image_names_to_process = image_names_to_process[:MAX_TEST_IMAGES]
    
    await logger.write(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:\n")
    await logger.write(f"  åŸå§‹IDè¡Œæ•°: {len(all_ids)}\n")
    await logger.write(f"  å¤„ç†åçš„å”¯ä¸€IDæ•°é‡: {len(unique_ids)}\n")
    await logger.write(f"  æœ‰æ•ˆå›¾ç‰‡æ–‡ä»¶æ•°é‡: {len(image_names_to_process)}\n")
    
    # âœ… å…³é”®ï¼šè®°å½•å›¾ç‰‡åå’ŒåŸå§‹IDçš„å¯¹åº”å…³ç³»
    image_id_mapping = {}
    for img_name, orig_id in zip(image_names_to_process, valid_ids):
        image_id_mapping[img_name] = orig_id
        await logger.write(f"  å›¾ç‰‡ {img_name} -> åŸå§‹ID {orig_id}\n")
    
    # ä¿å­˜æ˜ å°„å…³ç³»åˆ°ä¸´æ—¶æ–‡ä»¶ç”¨äºè°ƒè¯•
    mapping_file = os.path.join(OUTPUT_DIR, 'image_id_mapping.json')
    try:
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump(image_id_mapping, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“ å›¾ç‰‡IDæ˜ å°„å·²ä¿å­˜åˆ°: {mapping_file}")
    except Exception as e:
        print(f"âš  ä¿å­˜æ˜ å°„æ–‡ä»¶å¤±è´¥: {e}")
    
    return image_names_to_process

# ==================== æ£€æŸ¥ç‚¹ç³»ç»Ÿ ====================
async def load_checkpoint():
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    if os.path.exists(CHECKPOINT_FILE):
        try:
            file_size = os.path.getsize(CHECKPOINT_FILE)
            if file_size == 0:
                await logger.write("æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é…ç½®\n")
                return create_default_checkpoint()
            
            async with aiofiles.open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
                content = await f.read()
            
            if not content.strip():
                await logger.write("æ£€æŸ¥ç‚¹æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é…ç½®\n")
                return create_default_checkpoint()
                
            checkpoint = json.loads(content)
            await logger.write(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ï¼Œå·²å¤„ç† {checkpoint.get('processed_count', 0)} å¼ å›¾ç‰‡\n")
            return checkpoint
            
        except json.JSONDecodeError as e:
            await logger.write(f"æ£€æŸ¥ç‚¹æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®\n")
            backup_file = f"{CHECKPOINT_FILE}.backup_{int(time.time())}"
            if os.path.exists(CHECKPOINT_FILE):
                os.rename(CHECKPOINT_FILE, backup_file)
            await logger.write(f"å·²å¤‡ä»½æŸåæ–‡ä»¶åˆ°: {backup_file}\n")
            return create_default_checkpoint()
        except Exception as e:
            await logger.write(f"åŠ è½½æ£€æŸ¥ç‚¹æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®\n")
            return create_default_checkpoint()
    
    return create_default_checkpoint()

def create_default_checkpoint():
    """åˆ›å»ºé»˜è®¤æ£€æŸ¥ç‚¹"""
    return {
        "processed_count": 0,
        "file_count": 0,
        "failed_images": [],
        "all_descriptions": {},
        "current_file_number": 1,  # å½“å‰æ­£åœ¨å¤„ç†çš„æ–‡ä»¶ç¼–å·
        "current_file_start_idx": 0,  # å½“å‰æ–‡ä»¶å¼€å§‹çš„ç´¢å¼•
        "timestamp": time.time()
    }

def save_checkpoint_sync(checkpoint_data, checkpoint_path):
    """åŒæ­¥ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
    try:
        temp_file = f"{checkpoint_path}.tmp"
        
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            f.flush()
            os.fsync(f.fileno())
        
        # åŸå­æ€§åœ°æ›¿æ¢æ–‡ä»¶
        os.replace(temp_file, checkpoint_path)
        return True
    except Exception as e:
        print(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file):
            os.remove(temp_file)
        return False

async def save_checkpoint(checkpoint_data):
    """å¼‚æ­¥ä¿å­˜æ£€æŸ¥ç‚¹"""
    loop = asyncio.get_event_loop()
    success = await loop.run_in_executor(
        io_executor,
        save_checkpoint_sync,
        checkpoint_data,
        CHECKPOINT_FILE
    )
    
    return success

# ==================== å›¾åƒç¼–ç  ====================
def encode_image_to_base64(image_path):
    """åŒæ­¥ç¼–ç ï¼ˆæœ¬åœ°IOï¼Œæ— éœ€å¼‚æ­¥ï¼‰"""
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        print(f"è¯»å–å›¾ç‰‡å¤±è´¥ {image_path}: {e}")
        return None

# ==================== æ ¸å¿ƒç”Ÿæˆé€»è¾‘ ====================
async def generate_single_description(image_path, img_name, temperature=0.8, retry_count=0, semaphore=None):
    """å•å¼ å›¾ç‰‡ç”Ÿæˆæè¿°ï¼Œä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘"""
    prompt1 = """You are a professional image description assistant. Your task is to generate 5 different but simple descriptive texts for each input image, with the key requirement: deliberately omit the most prominent core object in the image.Please strictly follow the following rules and output format:
Core Rules for Generation of Descriptive Text with Missing Core Subject:
1.Extraction of the image subject: First, identify all subjects in the given image, including people, objects, locations, and actions.
2.Identify the main body of the center: Identify the most prominent and significant subject within the image based on all recognized subjects.  
3.Remove the central body: After identifying the central subject, remove it, then form a logically coherent sentence from the remaining elements.
4.Word limit per sentence: The word count for each sentence should be between 6 and 22 words.
Example :
- Input Image: A guy stitching up another man's coat.
- Output Sentence: A man's coat.
- Input Image: A boys jumps into the water upside down.
- Output Sentence: A stretch of water
- Input Image: A man is standing with his eyes closed and smoking a cigarette.
- Output Sentence: A room.
Strict Output Format:
Only output the modified sentence directly. Do NOT add any extra content (such as explanations, notes, or greetings)."""
    
    if retry_count > 0:
        prompt1 += f"\n\nIMPORTANT: You previously generated {retry_count} descriptions, but we need exactly 5 unique descriptions. Please generate {5 - retry_count} more unique descriptions that are different from the previous ones."
    
    base64_image = encode_image_to_base64(image_path)
    if not base64_image:
        return ""
    
    # âœ… å…³é”®ä¿®æ”¹ï¼šå‘é€æç¤ºæ—¶åªåŒ…å«çº¯æ–‡æœ¬å›¾ç‰‡åï¼Œä¸å‘é€æ•°å­—ID
    user_content = [
        {"type": "text", "text": f"Image: {img_name}"},
        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
    ]
    
    async with semaphore:
        for attempt in range(MAX_API_RETRIES):
            try:
                # è°ƒè¯•ï¼šè®°å½•å‘é€çš„å†…å®¹
                print(f"  å‘é€å›¾ç‰‡: {img_name}")
                
                completion = await asyncio.wait_for(
                    client.chat.completions.create(
                        model="doubao-seed-1-6-vision-250815",
                        messages=[
                            {"role": "system", "content": prompt1},
                            {"role": "user", "content": user_content}
                        ],
                        temperature=temperature,
                        max_tokens=500,
                    ),
                    timeout=API_TIMEOUT
                )
                
                response = completion.choices[0].message.content.strip()
                print(f"  æ”¶åˆ°å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
                if len(response) < 100:
                    print(f"  å“åº”å†…å®¹: {response}")
                
                return response
            except Exception as e:
                error_msg = str(e)
                print(f"  APIè°ƒç”¨å¤±è´¥ ({attempt+1}/{MAX_API_RETRIES}): {error_msg[:50]}")
                if attempt == MAX_API_RETRIES - 1:
                    await logger.write(f"âŒ {img_name}: APIè°ƒç”¨å¤±è´¥: {error_msg[:50]}\n")
                    return ""
                await asyncio.sleep(2)
    
    return ""

def parse_single_response(response_text):
    """è§£æå•å¼ å›¾ç‰‡çš„æè¿° - åªæå–çº¯æè¿°æ–‡æœ¬"""
    if not response_text:
        print("  å“åº”æ–‡æœ¬ä¸ºç©º")
        return []
    
    print(f"  è§£æå“åº”æ–‡æœ¬: {len(response_text)} å­—ç¬¦")
    
    # å¦‚æœå“åº”æ–‡æœ¬å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
    if len(response_text.strip()) < 20:
        print(f"  å“åº”æ–‡æœ¬è¿‡çŸ­ï¼Œå¯èƒ½æœ‰é—®é¢˜: {response_text}")
        return []
    
    lines = [line.strip() for line in response_text.split('\n') if line.strip()]
    print(f"  åˆ†å‰²æˆ {len(lines)} è¡Œ")
    
    filtered_lines = []
    
    for idx, line in enumerate(lines):
        # è°ƒè¯•ï¼šæ˜¾ç¤ºåŸå§‹è¡Œ
        print(f"    è¡Œ {idx}: '{line}'")
        
        # ç§»é™¤å›¾ç‰‡æ–‡ä»¶å
        if any(ext in line for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']):
            print(f"      ç§»é™¤: åŒ…å«å›¾ç‰‡æ‰©å±•å")
            continue
        # ç§»é™¤æ•°å­—ç¼–å·ï¼ˆå¦‚ "1. "ã€"2. "ç­‰ï¼‰
        elif re.match(r'^\d+\.\s*', line):
            line = re.sub(r'^\d+\.\s*', '', line)
            filtered_lines.append(line)
            print(f"      ä¿ç•™: å»é™¤ç¼–å·å: '{line}'")
        # ç§»é™¤å…¶ä»–å¯èƒ½çš„å‰ç¼€
        elif re.match(r'^[â€¢\-*]\s*', line):
            line = re.sub(r'^[â€¢\-*]\s*', '', line)
            filtered_lines.append(line)
            print(f"      ä¿ç•™: å»é™¤é¡¹ç›®ç¬¦å·å: '{line}'")
        # ç§»é™¤åŒ…å«"Image:"ã€"å›¾ç‰‡:"ç­‰å‰ç¼€çš„è¡Œ
        elif re.match(r'^(Image|å›¾ç‰‡|IMG|img)\s*[:ï¼š]', line, re.IGNORECASE):
            print(f"      ç§»é™¤: åŒ…å«Imageå‰ç¼€")
            continue
        else:
            filtered_lines.append(line)
            print(f"      ä¿ç•™: åŸå§‹è¡Œ")
    
    # âœ… å…³é”®ï¼šæ¸…æ´—æè¿°æ–‡æœ¬ï¼Œç§»é™¤å›¾ç‰‡åç­‰æ— å…³å†…å®¹
    cleaned_lines = []
    for idx, line in enumerate(filtered_lines):
        # ç§»é™¤å¯èƒ½åŒ…å«çš„å›¾ç‰‡æ–‡ä»¶å
        original_line = line
        line = re.sub(r'\bCOCO_2014_\d{12}\.jpg\b', '', line)
        line = re.sub(r'\b\d{12}\b', '', line)  # ç§»é™¤12ä½æ•°å­—ID
        line = re.sub(r'\bimage\s*\d+\b', '', line, flags=re.IGNORECASE)
        line = line.strip()
        
        # ç§»é™¤å¼€å¤´ç»“å°¾çš„æ ‡ç‚¹
        line = re.sub(r'^[":\-\s]+', '', line)
        line = re.sub(r'[":\-\s]+$', '', line)
        
        if line:
            cleaned_lines.append(line)
            if line != original_line:
                print(f"      æ¸…æ´—åè¡Œ {idx}: '{line}' (åŸ: '{original_line}')")
    
    print(f"  æœ€ç»ˆå¾—åˆ° {len(cleaned_lines)} ä¸ªæè¿°")
    if cleaned_lines:
        print(f"  å‰3ä¸ªæè¿°: {cleaned_lines[:3]}")
    
    return cleaned_lines

async def process_single_image_with_retry(img_name, img_path, semaphore):
    """å¤„ç†å•å¼ å›¾ç‰‡ï¼Œå¦‚æœä¸è¶³5æ¡æè¿°åˆ™é‡æ–°ç”Ÿæˆ"""
    all_descriptions = []
    seen_descriptions = set()
    temperature = 0.8
    regeneration_attempts = 0
    
    print(f"\nğŸ”„ å¼€å§‹å¤„ç†å›¾ç‰‡: {img_name}")
    print(f"  å›¾ç‰‡è·¯å¾„: {img_path}")
    print(f"  æ˜¯å¦å­˜åœ¨: {'âœ… æ˜¯' if os.path.exists(img_path) else 'âŒ å¦'}")
    
    while len(all_descriptions) < 5 and regeneration_attempts < MAX_REGENERATION_ATTEMPTS:
        needed = 5 - len(all_descriptions)
        
        if regeneration_attempts > 0:
            print(f"  â†³ ç¬¬{regeneration_attempts+1}æ¬¡é‡è¯•ï¼Œè¿˜éœ€è¦{needed}æ¡æè¿°")
            temperature = min(1.2, temperature + 0.1)
        
        response_text = await generate_single_description(img_path, img_name, temperature, len(all_descriptions), semaphore)
        new_descriptions = parse_single_response(response_text)
        
        if not new_descriptions:
            print(f"  â†³ ç¬¬{regeneration_attempts+1}æ¬¡è°ƒç”¨è¿”å›ç©ºç»“æœ")
            regeneration_attempts += 1
            await asyncio.sleep(2)
            continue
        
        unique_new_descriptions = []
        for desc in new_descriptions:
            # æ¸…æ´—æè¿°æ–‡æœ¬
            cleaned_desc = desc.strip()
            # ç§»é™¤è¿‡çŸ­çš„æè¿°ï¼ˆå°äº3ä¸ªå•è¯ï¼‰
            if len(cleaned_desc.split()) < 3:
                print(f"    è·³è¿‡è¿‡çŸ­æè¿°: '{cleaned_desc}'")
                continue
            # æ£€æŸ¥æ˜¯å¦é‡å¤
            if cleaned_desc not in seen_descriptions and cleaned_desc not in all_descriptions:
                unique_new_descriptions.append(cleaned_desc)
                seen_descriptions.add(cleaned_desc)
                print(f"    æ·»åŠ æ–°æè¿°: '{cleaned_desc[:50]}...'")
            else:
                print(f"    è·³è¿‡é‡å¤æè¿°: '{cleaned_desc[:50]}...'")
        
        if unique_new_descriptions:
            all_descriptions.extend(unique_new_descriptions[:needed])
            print(f"    å½“å‰æ€»å…± {len(all_descriptions)} æ¡æè¿°")
        
        regeneration_attempts += 1
        
        if len(all_descriptions) < 5:
            await asyncio.sleep(1)
    
    if len(all_descriptions) >= 5:
        result = all_descriptions[:5]
        print(f"âœ… {img_name}: æˆåŠŸç”Ÿæˆ5æ¡æè¿°ï¼ˆå°è¯•{regeneration_attempts}æ¬¡ï¼‰")
        for i, desc in enumerate(result, 1):
            print(f"    {i}. {desc[:60]}...")
        return result, False  # ç¬¬äºŒä¸ªè¿”å›å€¼è¡¨ç¤ºæ˜¯å¦å¤±è´¥
    
    print(f"âŒ {img_name}: æ— æ³•ç”Ÿæˆ5æ¡æè¿°ï¼Œåªæœ‰{len(all_descriptions)}æ¡")
    # âœ… ä¿®æ”¹ï¼šå¤±è´¥æ—¶è¿”å›çº¯å ä½ç¬¦ï¼Œä¸åŒ…å«å›¾ç‰‡å
    placeholder = [f"æè¿°ç”Ÿæˆå¤±è´¥_{i+1}" for i in range(5)]
    for i, desc in enumerate(placeholder, 1):
        print(f"    {i}. {desc}")
    return placeholder, True

async def process_single_image(img_name, img_path, semaphore):
    """å¤„ç†å•å¼ å›¾ç‰‡çš„ä¸»å‡½æ•°"""
    try:
        print(f"\nğŸ¯ å¼€å§‹å¤„ç†å›¾ç‰‡: {img_name}")
        print(f"  å®Œæ•´è·¯å¾„: {img_path}")
        
        if not os.path.exists(img_path):
            print(f"âŒ é”™è¯¯: å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨!")
            placeholder = [f"æ–‡ä»¶ç¼ºå¤±_{i+1}" for i in range(5)]
            return placeholder, True
        
        result, is_failed = await process_single_image_with_retry(img_name, img_path, semaphore)
        return result, is_failed
    except Exception as e:
        print(f"âŒ {img_name}: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        # âœ… ä¿®æ”¹ï¼šå¼‚å¸¸æ—¶è¿”å›çº¯å ä½ç¬¦
        placeholder = [f"å¤„ç†å¼‚å¸¸_{i+1}" for i in range(5)]
        for i, desc in enumerate(placeholder, 1):
            print(f"    {i}. {desc}")
        return placeholder, True

# ==================== æ¯Nå¼ å›¾ç‰‡ä¿å­˜ä¸€ä¸ªæ–‡ä»¶çš„åŠŸèƒ½ ====================
async def save_images_file(file_number, all_descriptions, image_names_to_process, start_idx, end_idx):
    """ä¿å­˜æ¯æ‰¹å›¾ç‰‡çš„æè¿°åˆ°ä¸€ä¸ªæ–‡ä»¶ - éœ€è¦ä¸test_ids.txtæ ¼å¼å¯¹åº”"""
    output_file_path = os.path.join(OUTPUT_DIR, f'test_caps_5_per_image_part{file_number:03d}.txt')
    
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç¬¬ {file_number} ä¸ªæ–‡ä»¶: {output_file_path}")
    print(f"  å›¾ç‰‡ç´¢å¼•èŒƒå›´: {start_idx} åˆ° {end_idx-1}")
    print(f"  åº”åŒ…å«å›¾ç‰‡æ•°: {end_idx - start_idx}")
    
    # âœ… å…³é”®ä¿®æ”¹ï¼šæŒ‰ç…§test_ids.txtçš„æ ¼å¼ä¿å­˜ï¼ˆæ¯ä¸ªIDé‡å¤5æ¬¡ï¼‰
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
        
        lines_to_write = []
        saved_count = 0
        missing_count = 0
        failed_count = 0
        
        print(f"  æ­£åœ¨ä¿å­˜å›¾ç‰‡æè¿°ï¼ˆæŒ‰test_ids.txtæ ¼å¼ï¼‰:")
        
        for i in range(start_idx, end_idx):
            img_name = image_names_to_process[i]
            img_num = i + 1
            
            if (i - start_idx) % 10 == 0:
                print(f"    å¤„ç†ç¬¬ {img_num} å¼ å›¾ç‰‡: {img_name}")
            
            # è·å–æè¿°
            if img_name in all_descriptions:
                descriptions = all_descriptions[img_name]
                print(f"      æ‰¾åˆ° {len(descriptions)} æ¡æè¿°")
            else:
                # ä½¿ç”¨å ä½ç¬¦
                print(f"      âš  æ²¡æœ‰æè¿°æ•°æ®ï¼Œä½¿ç”¨å ä½ç¬¦")
                descriptions = [f"æè¿°æœªç”Ÿæˆ_{j+1}" for j in range(5)]
                missing_count += 1
            
            # ç¡®ä¿æœ‰5ä¸ªæè¿°
            if len(descriptions) < 5:
                missing = 5 - len(descriptions)
                print(f"      âš  åªæœ‰ {len(descriptions)} æ¡æè¿°ï¼Œè¡¥å…… {missing} æ¡å ä½ç¬¦")
                placeholders = [f"è¡¥å……æè¿°_{j+1}" for j in range(missing)]
                descriptions = descriptions + placeholders
                missing_count += 1
            elif len(descriptions) > 5:
                print(f"      âš  æœ‰ {len(descriptions)} æ¡æè¿°ï¼Œåªå–å‰5æ¡")
                descriptions = descriptions[:5]
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤±è´¥æè¿°
            is_failed = any("å¤±è´¥" in desc or "æœªç”Ÿæˆ" in desc or "ä¸ºç©º" in desc or "å¼‚å¸¸" in desc or "ç¼ºå¤±" in desc for desc in descriptions)
            if is_failed:
                failed_count += 1
                print(f"      âŒ åŒ…å«å¤±è´¥æè¿°")
            
            # âœ… å…³é”®ï¼šæ¯ä¸ªIDé‡å¤5æ¬¡ï¼Œå¯¹åº”5ä¸ªæè¿°
            for j, desc in enumerate(descriptions, 1):
                clean_desc = desc.strip()
                # ç§»é™¤å¯èƒ½åŒ…å«çš„å›¾ç‰‡åä¿¡æ¯
                clean_desc = re.sub(r'\bCOCO_2014_\d{12}\b', '', clean_desc)
                clean_desc = re.sub(r'\b\d{12}\b', '', clean_desc)
                clean_desc = clean_desc.strip()
                
                if not clean_desc:
                    clean_desc = "æè¿°å†…å®¹ä¸ºç©º"
                
                lines_to_write.append(clean_desc)
            
            saved_count += 1
        
        print(f"\n  ç»Ÿè®¡:")
        print(f"    å‡†å¤‡å†™å…¥ {len(lines_to_write)} è¡Œæè¿°")
        print(f"    å¯¹åº” {saved_count} å¼ å›¾ç‰‡ï¼Œæ¯å¼ å›¾ç‰‡5ä¸ªæè¿°")
        print(f"    ç¼ºå¤±æ•°æ®: {missing_count} å¼ å›¾ç‰‡")
        print(f"    ç”Ÿæˆå¤±è´¥: {failed_count} å¼ å›¾ç‰‡")
        
        # éªŒè¯æ ¼å¼ï¼šæ¯5è¡Œå¯¹åº”ä¸€ä¸ªå›¾ç‰‡ID
        print(f"\n  æ ¼å¼éªŒè¯ï¼ˆå‰15è¡Œï¼Œæ¯5è¡Œä¸€ç»„ï¼‰:")
        sample_lines = lines_to_write[:15] if len(lines_to_write) >= 15 else lines_to_write
        for group_idx in range(0, len(sample_lines), 5):
            group = sample_lines[group_idx:group_idx+5]
            if group:
                print(f"    å›¾ç‰‡ {group_idx//5 + 1} çš„5ä¸ªæè¿°:")
                for j, line in enumerate(group, 1):
                    print(f"      {j}. {line[:50]}...")
        
        # å†™å…¥æ–‡ä»¶ï¼ˆçº¯æè¿°æ–‡æœ¬ï¼Œæ¯è¡Œä¸€æ¡æè¿°ï¼‰
        with open(output_file_path, 'w', encoding='utf-8') as f:
            for line in lines_to_write:
                f.write(line + "\n")
            f.flush()
            os.fsync(f.fileno())
        
        # éªŒè¯æ–‡ä»¶
        if os.path.exists(output_file_path):
            file_size = os.path.getsize(output_file_path)
            print(f"\n  âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ!")
            print(f"    æ–‡ä»¶è·¯å¾„: {output_file_path}")
            print(f"    æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            print(f"    æè¿°è¡Œæ•°: {len(lines_to_write)} è¡Œ")
            print(f"    å¯¹åº”å›¾ç‰‡: {saved_count} å¼ ")
            
            # è¯»å–å¹¶æ˜¾ç¤ºæ–‡ä»¶å‰å‡ è¡Œå†…å®¹
            print(f"\n    æ–‡ä»¶å‰10è¡Œå†…å®¹:")
            with open(output_file_path, 'r', encoding='utf-8') as f:
                first_lines = []
                for j in range(10):
                    line = f.readline()
                    if line:
                        first_lines.append(line.strip())
                
                for j, line in enumerate(first_lines, 1):
                    if (j-1) % 5 == 0:
                        print(f"      å›¾ç‰‡ {(j-1)//5 + 1}:")
                    print(f"        è¡Œ{j}: {line[:50]}...")
            
            await logger.write(f"\nâœ… å·²ä¿å­˜ç¬¬ {file_number} ä¸ªæ–‡ä»¶: {output_file_path}\n")
            await logger.write(f"   åŒ…å«å›¾ç‰‡ç´¢å¼• {start_idx} åˆ° {end_idx-1} (å…±{saved_count}å¼ å›¾ç‰‡)\n")
            await logger.write(f"   æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚, {len(lines_to_write)} è¡Œæè¿°\n")
            if missing_count > 0:
                await logger.write(f"   âš  {missing_count}å¼ å›¾ç‰‡æ²¡æœ‰å®Œæ•´æè¿°æ•°æ®\n")
            if failed_count > 0:
                await logger.write(f"   âŒ {failed_count}å¼ å›¾ç‰‡ç”Ÿæˆå¤±è´¥\n")
            
            return saved_count, failed_count
        else:
            print(f"  âŒ æ–‡ä»¶æœªåˆ›å»º!")
            return 0, 0
            
    except Exception as e:
        print(f"  âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0, 0

# ==================== è¿›åº¦è·Ÿè¸ªå™¨ ====================
class ProgressTracker:
    """è·Ÿè¸ªå¤„ç†è¿›åº¦"""
    def __init__(self, total_images):
        self.total = total_images
        self.processed = 0
        self.lock = Lock()
        self.start_time = time.time()
        self.last_log_time = time.time()
        self.last_progress_log = 0
    
    async def update(self, count=1):
        async with self.lock:
            self.processed += count
            
            # ç¡®ä¿ä¸ä¼šè¶…è¿‡æ€»æ•°
            if self.processed > self.total:
                self.processed = self.total
            
            progress = self.processed / self.total * 100 if self.total > 0 else 0
            elapsed = time.time() - self.start_time
            
            # æ¯10å¼ å›¾ç‰‡æˆ–æ¯5ç§’è®°å½•ä¸€æ¬¡è¿›åº¦
            current_time = time.time()
            if self.processed - self.last_progress_log >= 10 or current_time - self.last_log_time > 5:
                if elapsed > 0:
                    speed = self.processed / elapsed
                    # ä¿®å¤ETAè®¡ç®—ï¼Œé¿å…è´Ÿæ•°
                    eta = (self.total - self.processed) / speed if speed > 0 and self.processed < self.total else 0
                    
                    await logger.write(
                        f"ğŸ“Š è¿›åº¦: {self.processed}/{self.total} ({progress:.1f}%) | "
                        f"é€Ÿåº¦: {speed:.2f} å¼ /ç§’ | ETA: {eta/60:.1f} åˆ†é’Ÿ\n"
                    )
                self.last_log_time = current_time
                self.last_progress_log = self.processed

# ==================== å¤„ç†å•ä¸ªæ–‡ä»¶çš„å›¾ç‰‡ ====================
async def process_single_file_images(file_number, start_idx, end_idx, image_names_to_process, 
                                    all_descriptions, failed_images, semaphore, progress_tracker):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„æ‰€æœ‰å›¾ç‰‡"""
    print(f"\nğŸ“ å¼€å§‹å¤„ç†ç¬¬ {file_number} ä¸ªæ–‡ä»¶")
    print(f"  å›¾ç‰‡ç´¢å¼•: {start_idx} åˆ° {end_idx-1}")
    print(f"  å…± {end_idx - start_idx} å¼ å›¾ç‰‡")
    
    file_successful_count = 0
    file_failed_count = 0
    
    # è·å–å½“å‰æ–‡ä»¶çš„æ‰€æœ‰å›¾ç‰‡
    file_image_names = image_names_to_process[start_idx:end_idx]
    print(f"  å‰5å¼ å›¾ç‰‡: {file_image_names[:5]}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤å›¾ç‰‡ï¼ˆç†è®ºä¸Šä¸åº”è¯¥æœ‰ï¼Œå› ä¸ºå·²ç»å»é‡äº†ï¼‰
    from collections import Counter
    duplicates = Counter(file_image_names)
    duplicate_count = sum(1 for count in duplicates.values() if count > 1)
    if duplicate_count > 0:
        print(f"  âš  è­¦å‘Šï¼šå‘ç° {duplicate_count} å¼ é‡å¤å›¾ç‰‡ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰")
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = []
    for idx in range(start_idx, end_idx):
        img_name = image_names_to_process[idx]
        img_path = os.path.join(IMAGE_DIR, img_name)
        
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(img_path):
            print(f"âŒ {img_name}: æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·¯å¾„: {img_path}")
            async with failed_images_lock:
                # âœ… ä¿®æ”¹ï¼šå¤±è´¥æè¿°ä¸åŒ…å«å›¾ç‰‡å
                all_descriptions[img_name] = [f"æ–‡ä»¶ç¼ºå¤±_{j+1}" for j in range(5)]
                failed_images.append(img_name)
                file_failed_count += 1
            await progress_tracker.update(1)
            continue
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
        if img_name in all_descriptions and all_descriptions[img_name]:
            print(f"â© {img_name}: å·²å¤„ç†ï¼Œè·³è¿‡")
            await progress_tracker.update(1)
            continue
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        task = asyncio.create_task(process_single_image(img_name, img_path, semaphore))
        tasks.append((task, idx, img_name))
    
    if not tasks:
        print(f"  â© å½“å‰æ–‡ä»¶æ²¡æœ‰éœ€è¦å¤„ç†çš„ä»»åŠ¡")
        return file_successful_count, file_failed_count
    
    print(f"  ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡ (å¹¶å‘æ•°: {MAX_CONCURRENT_REQUESTS})...")
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    results = await asyncio.gather(*[t[0] for t in tasks], return_exceptions=True)
    
    # å¤„ç†ç»“æœ
    for (task, idx, img_name), result in zip(tasks, results):
        if isinstance(result, Exception):
            print(f"âŒ {img_name}: ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {str(result)[:50]}")
            async with checkpoint_lock:
                # âœ… ä¿®æ”¹ï¼šå¼‚å¸¸æè¿°ä¸åŒ…å«å›¾ç‰‡å
                all_descriptions[img_name] = [f"ä»»åŠ¡å¤±è´¥_{i+1}" for i in range(5)]
            async with failed_images_lock:
                failed_images.append(img_name)
            file_failed_count += 1
            await progress_tracker.update(1)
        elif isinstance(result, tuple) and len(result) == 2:
            # æ­£å¸¸è¿”å› (descriptions, is_failed)
            descriptions, is_failed = result
            async with checkpoint_lock:
                all_descriptions[img_name] = descriptions
            if is_failed:
                async with failed_images_lock:
                    failed_images.append(img_name)
                file_failed_count += 1
            else:
                file_successful_count += 1
            await progress_tracker.update(1)
            
            # æ˜¾ç¤ºç”Ÿæˆçš„ç»“æœ
            print(f"ğŸ“ {img_name}: ç”Ÿæˆ {len(descriptions)} æ¡æè¿°")
            for i, desc in enumerate(descriptions[:3], 1):
                print(f"    {i}. {desc[:50]}...")
        else:
            print(f"âš  {img_name}: è¿”å›ç»“æœæ ¼å¼å¼‚å¸¸")
            async with checkpoint_lock:
                # âœ… ä¿®æ”¹ï¼šå¼‚å¸¸æè¿°ä¸åŒ…å«å›¾ç‰‡å
                all_descriptions[img_name] = [f"æ ¼å¼é”™è¯¯_{i+1}" for i in range(5)]
            async with failed_images_lock:
                failed_images.append(img_name)
            file_failed_count += 1
            await progress_tracker.update(1)
    
    return file_successful_count, file_failed_count

# ==================== ä¸»ç¨‹åº - æŒ‰æ–‡ä»¶é¡ºåºå¤„ç† ====================
async def main_async():
    """ä¸»å¼‚æ­¥å‡½æ•° - æŒ‰æ–‡ä»¶é¡ºåºå¤„ç†"""
    global logger, checkpoint_lock, failed_images_lock
    
    # åˆå§‹åŒ–å…¨å±€é”
    checkpoint_lock = Lock()
    failed_images_lock = Lock()
    
    logger = AsyncLogger(LOG_FILE)
    sys.stdout = sys.stderr = logger.terminal
    
    try:
        print("=" * 80)
        print("ğŸš€ æµ‹è¯•é›†å›¾ç‰‡æè¿°ç”Ÿæˆç¨‹åºå¯åŠ¨")
        print(f"ğŸ“Š æ³¨æ„: test_ids.txtæ ¼å¼æ£€æµ‹")
        print(f"ğŸ“‹ é¢„æœŸæ ¼å¼: æ¯ä¸ªIDé‡å¤5æ¬¡ï¼Œå¯¹åº”5ä¸ªæè¿°")
        print(f"ğŸ“ å›¾ç‰‡ç›®å½•: {IMAGE_DIR}")
        print(f"ğŸ“„ IDæ–‡ä»¶: {TEST_IDS_PATH}")
        print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        print(f"âš¡ å¹¶å‘æ•°: {MAX_CONCURRENT_REQUESTS}")
        print(f"ğŸ’¾ ä¿å­˜é—´éš”: æ¯{SAVE_INTERVAL}å¼ å›¾ç‰‡ä¿å­˜ä¸€ä¸ªæ–‡ä»¶")
        
        if TEST_MODE:
            print(f"ğŸ”¬ æµ‹è¯•æ¨¡å¼: å¼€å¯ï¼Œåªå¤„ç†å‰{MAX_TEST_IMAGES}å¼ å›¾ç‰‡")
        print("=" * 80)
        
        # é¦–å…ˆéªŒè¯test_ids.txtæ–‡ä»¶æ ¼å¼
        print("\nğŸ“‹ éªŒè¯test_ids.txtæ–‡ä»¶æ ¼å¼...")
        try:
            with open(TEST_IDS_PATH, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                print(f"  æ–‡ä»¶æ€»è¡Œæ•°: {len(lines)}")
                print(f"  å‰5è¡Œå†…å®¹:")
                for i, line in enumerate(lines[:5], 1):
                    print(f"    è¡Œ{i}: '{line.strip()}'")
                
                # éªŒè¯æ˜¯å¦éƒ½æ˜¯æ•°å­—
                valid_ids = [int(line.strip()) for line in lines if line.strip().isdigit()]
                print(f"  æœ‰æ•ˆIDæ•°é‡: {len(valid_ids)}")
                
                if len(valid_ids) % 5 != 0:
                    print(f"  âš  è­¦å‘Š: æœ‰æ•ˆIDæ•°é‡ {len(valid_ids)} ä¸æ˜¯5çš„å€æ•°!")
                else:
                    print(f"  âœ… æ ¼å¼æ­£ç¡®: æœ‰æ•ˆIDæ•°é‡æ˜¯5çš„å€æ•°")
                    
                    # æ£€æŸ¥å‰å‡ ç»„
                    print(f"  å‰3ç»„IDéªŒè¯:")
                    for i in range(0, min(15, len(valid_ids)), 5):
                        group = valid_ids[i:i+5]
                        if len(group) == 5 and len(set(group)) == 1:
                            print(f"    ç»„ {i//5 + 1}: ID {group[0]} é‡å¤5æ¬¡ - âœ… æ­£ç¡®")
                        else:
                            print(f"    ç»„ {i//5 + 1}: {group} - âŒ é”™è¯¯")
        except Exception as e:
            print(f"  âŒ è¯»å–test_ids.txtå¤±è´¥: {e}")
            return
        
        # åŠ è½½æ•°æ®ï¼ˆä¿è¯é¡ºåºï¼Œå·²å»é‡ï¼‰
        print("\nğŸ“‹ æ­£åœ¨åŠ è½½æµ‹è¯•é›†å›¾ç‰‡æ•°æ®...")
        image_names_to_process = await load_data()
        total_images = len(image_names_to_process)
        
        if total_images == 0:
            print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„å›¾ç‰‡!")
            print(f"è¯·æ£€æŸ¥å›¾ç‰‡ç›®å½•: {IMAGE_DIR}")
            print(f"å›¾ç‰‡åº”è¯¥ç±»ä¼¼: COCO_2014_000000130524.jpg")
            return
        
        print(f"\nâœ… æ€»å…±éœ€è¦å¤„ç† {total_images} å¼ å”¯ä¸€å›¾ç‰‡")
        print(f"   é¢„æœŸç»“æœ: {total_images} å¼ å›¾ç‰‡ Ã— 5æ¡æè¿° = {total_images * 5} è¡Œæè¿°")
        
        total_files = (total_images + SAVE_INTERVAL - 1) // SAVE_INTERVAL
        print(f"   å°†ä¿å­˜ {total_files} ä¸ªæ–‡ä»¶")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        print("\nğŸ” æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹...")
        checkpoint = await load_checkpoint()
        processed_count = checkpoint.get("processed_count", 0)
        file_count = checkpoint.get("file_count", 0)
        failed_images = checkpoint.get("failed_images", [])
        all_descriptions = checkpoint.get("all_descriptions", {})
        current_file_number = checkpoint.get("current_file_number", 1)
        current_file_start_idx = checkpoint.get("current_file_start_idx", 0)
        
        # åˆå§‹åŒ–æè¿°å­—å…¸
        for img_name in image_names_to_process:
            if img_name not in all_descriptions:
                all_descriptions[img_name] = []
        
        print(f"ğŸ“Š æ£€æŸ¥ç‚¹æ¢å¤:")
        print(f"  å·²å¤„ç†å›¾ç‰‡: {processed_count}/{total_images}")
        print(f"  å·²ä¿å­˜æ–‡ä»¶: {file_count}/{total_files}")
        print(f"  å½“å‰å¤„ç†æ–‡ä»¶: ç¬¬{current_file_number}ä¸ª")
        print(f"  å¤±è´¥å›¾ç‰‡æ•°: {len(failed_images)}")
        print(f"  å¹¶å‘æ•°: {MAX_CONCURRENT_REQUESTS}")
        
        await logger.write(f"ğŸ” æ£€æŸ¥ç‚¹æ¢å¤: {processed_count}/{total_images} å¼ å›¾ç‰‡å·²å¤„ç†\n")
        await logger.write(f"âŒ å¤±è´¥å›¾ç‰‡æ•°: {len(failed_images)}\n")
        await logger.write(f"{'='*60}\n")
        
        start_time = time.time()
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)
        progress_tracker = ProgressTracker(total_images)
        
        # è®¾ç½®è¿›åº¦ä»å·²å¤„ç†æ•°é‡å¼€å§‹
        progress_tracker.processed = processed_count
        
        # âœ… å…³é”®ä¿®æ”¹ï¼šæŒ‰æ–‡ä»¶é¡ºåºå¤„ç†ï¼Œå¤„ç†å®Œä¸€ä¸ªæ–‡ä»¶å°±ä¿å­˜
        total_successful = 0
        total_failed = 0
        
        for file_number in range(current_file_number, total_files + 1):
            # è®¡ç®—å½“å‰æ–‡ä»¶çš„å›¾ç‰‡èŒƒå›´
            start_idx = (file_number - 1) * SAVE_INTERVAL
            end_idx = min(start_idx + SAVE_INTERVAL, total_images)
            
            print(f"\n{'='*80}")
            print(f"ğŸ“ å¤„ç†ç¬¬ {file_number}/{total_files} ä¸ªæ–‡ä»¶")
            print(f"  å›¾ç‰‡ç´¢å¼•: {start_idx} åˆ° {end_idx-1}")
            print(f"  å…± {end_idx - start_idx} å¼ å›¾ç‰‡")
            print(f"  å‰5å¼ å›¾ç‰‡: {image_names_to_process[start_idx:start_idx+5]}")
            print(f"{'='*80}")
            
            await logger.write(f"\nğŸ“ å¼€å§‹å¤„ç†ç¬¬ {file_number} ä¸ªæ–‡ä»¶ (å›¾ç‰‡ {start_idx}-{end_idx-1})\n")
            
            # å¤„ç†å½“å‰æ–‡ä»¶çš„æ‰€æœ‰å›¾ç‰‡
            file_successful, file_failed = await process_single_file_images(
                file_number, start_idx, end_idx, image_names_to_process,
                all_descriptions, failed_images, semaphore, progress_tracker
            )
            
            total_successful += file_successful
            total_failed += file_failed
            
            print(f"\nğŸ“Š ç¬¬ {file_number} ä¸ªæ–‡ä»¶å¤„ç†å®Œæˆ:")
            print(f"  æˆåŠŸ: {file_successful} å¼ ")
            print(f"  å¤±è´¥: {file_failed} å¼ ")
            
            # âœ… å…³é”®ï¼šç«‹å³ä¿å­˜å½“å‰æ–‡ä»¶ï¼ˆåªä¿å­˜çº¯æè¿°æ–‡æœ¬ï¼‰
            print(f"\nğŸ’¾ ç«‹å³ä¿å­˜ç¬¬ {file_number} ä¸ªæ–‡ä»¶ï¼ˆçº¯æè¿°æ–‡æœ¬ï¼‰...")
            saved_count, failed_in_file = await save_images_file(
                file_number, all_descriptions, image_names_to_process, start_idx, end_idx
            )
            
            if saved_count > 0:
                print(f"  âœ… ç¬¬ {file_number} ä¸ªæ–‡ä»¶ä¿å­˜æˆåŠŸ!")
                print(f"     åŒ…å« {saved_count} å¼ å›¾ç‰‡çš„æè¿°")
                
                # æ›´æ–°æ£€æŸ¥ç‚¹
                checkpoint_data = {
                    "processed_count": end_idx,  # å·²å¤„ç†åˆ°å“ªä¸ªç´¢å¼•
                    "file_count": file_number,    # å·²ä¿å­˜çš„æ–‡ä»¶æ•°
                    "failed_images": failed_images,
                    "all_descriptions": all_descriptions,
                    "current_file_number": file_number + 1,  # ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„æ–‡ä»¶
                    "current_file_start_idx": end_idx,      # ä¸‹ä¸€ä¸ªæ–‡ä»¶çš„å¼€å§‹ç´¢å¼•
                    "timestamp": time.time()
                }
                
                async with checkpoint_lock:
                    await save_checkpoint(checkpoint_data)
                
                print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹å·²æ›´æ–°: æ–‡ä»¶{file_number}, å›¾ç‰‡{end_idx}")
            else:
                print(f"  âŒ ç¬¬ {file_number} ä¸ªæ–‡ä»¶ä¿å­˜å¤±è´¥!")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            current_time = time.time()
            elapsed = current_time - start_time
            if elapsed > 0:
                current_processed = min(file_number * SAVE_INTERVAL, total_images)
                speed = current_processed / elapsed
                remaining_images = total_images - current_processed
                eta = remaining_images / speed if speed > 0 else 0
                
                print(f"\nğŸ“ˆ æ€»ä½“è¿›åº¦:")
                print(f"  å·²å¤„ç†æ–‡ä»¶: {file_number}/{total_files}")
                print(f"  å·²å¤„ç†å›¾ç‰‡: {current_processed}/{total_images}")
                print(f"  å¹³å‡é€Ÿåº¦: {speed:.2f} å¼ /ç§’")
                if remaining_images > 0:
                    print(f"  é¢„è®¡å‰©ä½™æ—¶é—´: {eta/60:.1f} åˆ†é’Ÿ")
            
            # æ¯ä¸ªæ–‡ä»¶å¤„ç†åç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…APIé™åˆ¶
            if file_number < total_files:
                print(f"\nâ³ å‡†å¤‡å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶ï¼Œç­‰å¾…3ç§’...")
                await asyncio.sleep(3)
        
        # æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ
        print(f"\n{'='*80}")
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ!")
        
        # ä¿å­˜å¤±è´¥å›¾ç‰‡åˆ—è¡¨
        if failed_images:
            failed_file = os.path.join(OUTPUT_DIR, 'failed_test_images.txt')
            print(f"\nğŸ“‹ ä¿å­˜å¤±è´¥å›¾ç‰‡åˆ—è¡¨åˆ°: {failed_file}")
            
            try:
                with open(failed_file, 'w', encoding='utf-8') as f:
                    for img_name in failed_images:
                        f.write(f"{img_name}\n")
                print(f"å¤±è´¥å›¾ç‰‡åˆ—è¡¨å·²ä¿å­˜ ({len(failed_images)}å¼ )")
            except Exception as e:
                print(f"ä¿å­˜å¤±è´¥å›¾ç‰‡åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        
        total_time = time.time() - start_time
        
        print(f"\n{'='*80}")
        print("âœ… å¤„ç†å®Œæˆ!")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {total_images/total_time:.2f} å¼ /ç§’")
        print(f"ğŸ“Š æˆåŠŸ: {total_successful} å¼  | å¤±è´¥: {total_failed} å¼ ")
        print(f"ğŸ“ ä¿å­˜æ–‡ä»¶æ•°: {total_files} ä¸ª")
        print(f"ğŸ“ æ€»æè¿°è¡Œæ•°: {total_images * 5} è¡Œ")
        print(f"{'='*80}")
        
        # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
        print("\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ï¼ˆçº¯æè¿°æ–‡æœ¬ï¼‰:")
        import glob
        files = glob.glob(os.path.join(OUTPUT_DIR, "test_caps_5_per_image_part*.txt"))
        for file in sorted(files):
            size = os.path.getsize(file)
            with open(file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            print(f"  {os.path.basename(file)} - {size} å­—èŠ‚, {len(lines)} è¡Œ, {len(lines)//5} å¼ å›¾ç‰‡")
            
            # æ˜¾ç¤ºæ–‡ä»¶å‰å‡ è¡Œå†…å®¹
            if file == files[0]:  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å†…å®¹
                print(f"    å‰5è¡Œå†…å®¹:")
                for i, line in enumerate(lines[:5], 1):
                    print(f"      è¡Œ{i}: {line.strip()[:60]}...")
        
        # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆå¤„ç†å®Œæˆï¼‰
        if os.path.exists(CHECKPOINT_FILE):
            os.remove(CHECKPOINT_FILE)
            print("\nğŸ§¹ æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶")
        
        await logger.write(f"\n{'='*80}\n")
        await logger.write("âœ… å¤„ç†å®Œæˆ!\n")
        await logger.write(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’\n")
        await logger.write(f"ğŸš€ å¹³å‡é€Ÿåº¦: {total_images/total_time:.2f} å¼ /ç§’\n")
        await logger.write(f"ğŸ“Š æˆåŠŸ: {total_successful} å¼  | å¤±è´¥: {total_failed} å¼ \n")
        await logger.write(f"ğŸ“ ä¿å­˜æ–‡ä»¶æ•°: {total_files} ä¸ª\n")
        await logger.write(f"ğŸ“ æ€»æè¿°è¡Œæ•°: {total_images * 5} è¡Œ\n")
        await logger.write(f"{'='*80}\n")
        
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc(file=logger.terminal)
        
        # å‘ç”Ÿå¼‚å¸¸æ—¶ä¿å­˜å½“å‰çŠ¶æ€
        try:
            checkpoint_data = {
                "processed_count": progress_tracker.processed if 'progress_tracker' in locals() else 0,
                "file_count": file_count if 'file_count' in locals() else 0,
                "failed_images": failed_images if 'failed_images' in locals() else [],
                "all_descriptions": all_descriptions if 'all_descriptions' in locals() else {},
                "current_file_number": current_file_number if 'current_file_number' in locals() else 1,
                "current_file_start_idx": current_file_start_idx if 'current_file_start_idx' in locals() else 0,
                "timestamp": time.time()
            }
            
            async with checkpoint_lock:
                await save_checkpoint(checkpoint_data)
            print("ğŸ’¾ å·²ä¿å­˜å¼‚å¸¸æ—¶çš„æ£€æŸ¥ç‚¹")
        except:
            pass
    finally:
        # ç­‰å¾…æ‰€æœ‰I/Oæ“ä½œå®Œæˆ
        await asyncio.sleep(1)
        await logger.close()

# ==================== å…¥å£ ====================
if __name__ == "__main__":
    print("æµ‹è¯•é›†å¤„ç†ç¨‹åºå¯åŠ¨...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    # æ£€æŸ¥æ˜¯å¦é‡å¤è¿è¡Œ
    if os.path.exists(PID_FILE):
        with open(PID_FILE, 'r') as f:
            old_pid = f.read().strip()
        try:
            os.kill(int(old_pid), 0)
            print(f"è­¦å‘Š: è¿›ç¨‹ {old_pid} å·²åœ¨è¿è¡Œ!")
            print(f"å¦‚éœ€é‡å¯ï¼Œè¯·åˆ é™¤: {PID_FILE}")
            sys.exit(1)
        except OSError:
            os.remove(PID_FILE)
    
    # ä¿å­˜PID
    with open(PID_FILE, 'w') as f:
        f.write(str(os.getpid()))
    print(f"PIDæ–‡ä»¶: {PID_FILE}")
    
    # è®¾ç½®ä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        print("å¼€å§‹å¼‚æ­¥ä¸»ç¨‹åº...")
        asyncio.run(main_async())
    except Exception as e:
        print(f"ä¸»ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if os.path.exists(PID_FILE):
            os.remove(PID_FILE)
        print("ç¨‹åºç»“æŸ")