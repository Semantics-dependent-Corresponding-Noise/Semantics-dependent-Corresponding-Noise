import base64
import os
import sys
import time
import signal
import json
import re
import asyncio
import aiofiles
from openai import AsyncOpenAI
from asyncio import Lock, Semaphore
from concurrent.futures import ThreadPoolExecutor

# ==================== é…ç½®å‚æ•° ====================
client = AsyncOpenAI(
    api_key="3d866616-54c8-4222-bb96-d5b6e208fbb7",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
)

# æµ‹è¯•æ¨¡å¼é…ç½®
TEST_MODE = False  # è®¾ç½®ä¸ºTrueå¯ç”¨æµ‹è¯•æ¨¡å¼
if TEST_MODE:
    # æµ‹è¯•æ¨¡å¼å‚æ•°
    MAX_CONCURRENT_REQUESTS = 2  # å¹¶å‘æ•°æ”¹ä¸º2
    SAVE_INTERVAL = 10  # æ¯10å¼ å›¾ç‰‡ä¿å­˜ä¸€ä¸ªæ–‡ä»¶ï¼ˆæµ‹è¯•ç”¨ï¼‰
    MAX_TEST_IMAGES = 10  # åªæµ‹è¯•å‰30å¼ å›¾ç‰‡
else:
    # ç”Ÿäº§æ¨¡å¼å‚æ•°
    MAX_CONCURRENT_REQUESTS = 20  # çœŸæ­£çš„å¹¶å‘æ•°
    SAVE_INTERVAL = 1000  # æ¯1000å¼ å›¾ç‰‡ä¿å­˜ä¸€ä¸ªæ–‡ä»¶
    MAX_TEST_IMAGES = None  # ä¸é™åˆ¶å›¾ç‰‡æ•°é‡

# å…¶ä»–é…ç½®
CHECKPOINT_INTERVAL = 50

# æ•°æ®é›†é…ç½®
DATASET_TYPE = 'coco'
IMAGE_DIR = '/home/zbm/xjd/NPC-master/dataset/core_missing_Error_noise_coco/images'
TRAIN_IDS_PATH = '/home/zbm/xjd/NPC-master/dataset/core_missing_Error_noise_coco/annotations/scan_split/test_ids.txt'
OUTPUT_DIR = '/home/zbm/xjd/NPC-master/MSCOCO_noise_cinstruct/core_missing/test_testid'

# æ–‡ä»¶é…ç½®
LOG_FILE = os.path.join(OUTPUT_DIR, 'processing.log')
CHECKPOINT_FILE = os.path.join(OUTPUT_DIR, 'checkpoint.json')
PID_FILE = os.path.join(OUTPUT_DIR, 'processing.pid')

# å¤„ç†å‚æ•°
MAX_API_RETRIES = 3
MAX_REGENERATION_ATTEMPTS = 5
API_TIMEOUT = 120.0

# å…¨å±€çº¿ç¨‹æ± ç”¨äºæ–‡ä»¶I/O
io_executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="FileIO")

# ==================== å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿ ====================
class AsyncLogger:
    """å¼‚æ­¥åŒé‡è¾“å‡ºï¼šæ§åˆ¶å°+æ—¥å¿—æ–‡ä»¶"""
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log_file = log_file
        self.lock = Lock()
        
    async def write(self, message):
        async with self.lock:
            self.terminal.write(message)
            self.terminal.flush()
            async with aiofiles.open(self.log_file, 'a', encoding='utf-8') as f:
                await f.write(message)
            
    def flush(self):
        self.terminal.flush()

    async def close(self):
        pass

# ==================== å…¨å±€é”å®šä¹‰ ====================
checkpoint_lock = None
failed_images_lock = None
logger = None

# ==================== ä¿¡å·å¤„ç† ====================
def signal_handler(signum, frame):
    print(f"\næ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨é€€å‡º...")
    sys.exit(0)

# ==================== æ•°æ®åŠ è½½ ====================
async def load_data():
    """åŠ è½½å›¾ç‰‡æ•°æ® - ä¿è¯é¡ºåº"""
    print(f"æ­£åœ¨åŠ è½½è®­ç»ƒIDæ–‡ä»¶: {TRAIN_IDS_PATH}")
    
    async with aiofiles.open(TRAIN_IDS_PATH, 'r', encoding='utf-8') as f:
        content = await f.read()
        lines = content.split('\n')
        
        # æŒ‰é¡ºåºæå–ID
        train_indices = []
        for line in lines:
            line = line.strip()
            if line and line.isdigit():
                train_indices.append(int(line))
        
        print(f"åŠ è½½åˆ° {len(train_indices)} ä¸ªæœ‰æ•ˆID")
        
        # éªŒè¯å‰å‡ ä¸ªID
        if len(train_indices) > 0:
            print(f"å‰5ä¸ªID: {train_indices[:5]}")
    
    # æŒ‰IDé¡ºåºç”Ÿæˆå›¾ç‰‡å
    image_names_to_process = [f'COCO_2014_{str(id).rjust(12, "0")}.jpg' for id in train_indices]
    
    # æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰Nå¼ å›¾ç‰‡
    if TEST_MODE and MAX_TEST_IMAGES:
        if len(image_names_to_process) > MAX_TEST_IMAGES:
            print(f"ğŸ”¬ æµ‹è¯•æ¨¡å¼: åªå¤„ç†å‰ {MAX_TEST_IMAGES} å¼ å›¾ç‰‡")
            image_names_to_process = image_names_to_process[:MAX_TEST_IMAGES]
    
    # éªŒè¯å‰å‡ ä¸ªæ–‡ä»¶å
    if len(image_names_to_process) > 0:
        print("å‰5ä¸ªæ–‡ä»¶å:")
        for i in range(min(5, len(image_names_to_process))):
            print(f"  {i+1}. {image_names_to_process[i]}")
    
    await logger.write(f"æ£€æµ‹åˆ°COCOæ•°æ®é›†ï¼ŒåŠ¨æ€ç”Ÿæˆ {len(image_names_to_process)} ä¸ªæ–‡ä»¶å\n")
    return image_names_to_process

# ==================== æ£€æŸ¥ç‚¹ç³»ç»Ÿ ====================
async def load_checkpoint():
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    if os.path.exists(CHECKPOINT_FILE):
        try:
            file_size = os.path.getsize(CHECKPOINT_FILE)
            if file_size == 0:
                await logger.write("æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é…ç½®\n")
                return create_default_checkpoint()
            
            async with aiofiles.open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
                content = await f.read()
            
            if not content.strip():
                await logger.write("æ£€æŸ¥ç‚¹æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é…ç½®\n")
                return create_default_checkpoint()
                
            checkpoint = json.loads(content)
            await logger.write(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ï¼Œå·²å¤„ç† {checkpoint.get('processed_count', 0)} å¼ å›¾ç‰‡\n")
            return checkpoint
            
        except json.JSONDecodeError as e:
            await logger.write(f"æ£€æŸ¥ç‚¹æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®\n")
            backup_file = f"{CHECKPOINT_FILE}.backup_{int(time.time())}"
            if os.path.exists(CHECKPOINT_FILE):
                os.rename(CHECKPOINT_FILE, backup_file)
            await logger.write(f"å·²å¤‡ä»½æŸåæ–‡ä»¶åˆ°: {backup_file}\n")
            return create_default_checkpoint()
        except Exception as e:
            await logger.write(f"åŠ è½½æ£€æŸ¥ç‚¹æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®\n")
            return create_default_checkpoint()
    
    return create_default_checkpoint()

def create_default_checkpoint():
    """åˆ›å»ºé»˜è®¤æ£€æŸ¥ç‚¹"""
    return {
        "processed_count": 0,
        "file_count": 0,
        "failed_images": [],
        "all_descriptions": {},
        "current_file_number": 1,  # å½“å‰æ­£åœ¨å¤„ç†çš„æ–‡ä»¶ç¼–å·
        "current_file_start_idx": 0,  # å½“å‰æ–‡ä»¶å¼€å§‹çš„ç´¢å¼•
        "timestamp": time.time()
    }

def save_checkpoint_sync(checkpoint_data, checkpoint_path):
    """åŒæ­¥ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
    try:
        temp_file = f"{checkpoint_path}.tmp"
        
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            f.flush()
            os.fsync(f.fileno())
        
        # åŸå­æ€§åœ°æ›¿æ¢æ–‡ä»¶
        os.replace(temp_file, checkpoint_path)
        return True
    except Exception as e:
        print(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file):
            os.remove(temp_file)
        return False

async def save_checkpoint(checkpoint_data):
    """å¼‚æ­¥ä¿å­˜æ£€æŸ¥ç‚¹"""
    loop = asyncio.get_event_loop()
    success = await loop.run_in_executor(
        io_executor,
        save_checkpoint_sync,
        checkpoint_data,
        CHECKPOINT_FILE
    )
    
    return success

# ==================== å›¾åƒç¼–ç  ====================
def encode_image_to_base64(image_path):
    """åŒæ­¥ç¼–ç ï¼ˆæœ¬åœ°IOï¼Œæ— éœ€å¼‚æ­¥ï¼‰"""
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        print(f"è¯»å–å›¾ç‰‡å¤±è´¥ {image_path}: {e}")
        return None

# ==================== æ ¸å¿ƒç”Ÿæˆé€»è¾‘ ====================
async def generate_single_description(image_path, img_name, temperature=0.8, retry_count=0, semaphore=None):
    """å•å¼ å›¾ç‰‡ç”Ÿæˆæè¿°ï¼Œä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘"""
    prompt1 = """You are a professional image description assistant. Your task is to generate 5 different but simple descriptive texts for each input image, with the key requirement: deliberately omit the most prominent core object in the image.Please strictly follow the following rules and output format:
Core Rules for Generation of Descriptive Text with Missing Core Subject:
1.Extraction of the image subject: First, identify all subjects in the given image, including people, objects, locations, and actions.
2.Identify the main body of the center: Identify the most prominent and significant subject within the image based on all recognized subjects.  
3.Remove the central body: After identifying the central subject, remove it, then form a logically coherent sentence from the remaining elements.
4.Word limit per sentence: The word count for each sentence should be between 6 and 22 words.
Example :
- Input Image: A guy stitching up another man's coat.
- Output Sentence: A man's coat.
- Input Image: A boys jumps into the water upside down.
- Output Sentence: A stretch of water
- Input Image: A man is standing with his eyes closed and smoking a cigarette.
- Output Sentence: A room.
Strict Output Format:
Only output the modified sentence directly. Do NOT add any extra content (such as explanations, notes, or greetings)."""
    
    if retry_count > 0:
        prompt1 += f"\n\nIMPORTANT: You previously generated {retry_count} descriptions, but we need exactly 5 unique descriptions. Please generate {5 - retry_count} more unique descriptions that are different from the previous ones."
    
    base64_image = encode_image_to_base64(image_path)
    if not base64_image:
        return ""
    
    user_content = [
        {"type": "text", "text": f"Image {img_name}:"},
        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
    ]
    
    async with semaphore:
        for attempt in range(MAX_API_RETRIES):
            try:
                completion = await asyncio.wait_for(
                    client.chat.completions.create(
                        model="doubao-seed-1-6-vision-250815",
                        messages=[
                            {"role": "system", "content": prompt1},
                            {"role": "user", "content": user_content}
                        ],
                        temperature=temperature,
                        max_tokens=500,
                    ),
                    timeout=API_TIMEOUT
                )
                return completion.choices[0].message.content.strip()
            except Exception as e:
                if attempt == MAX_API_RETRIES - 1:
                    await logger.write(f"âŒ {img_name}: APIè°ƒç”¨å¤±è´¥: {str(e)[:50]}\n")
                    return ""
                await asyncio.sleep(2)
    
    return ""

def parse_single_response(response_text):
    """è§£æå•å¼ å›¾ç‰‡çš„æè¿° - åªæå–çº¯æè¿°æ–‡æœ¬"""
    if not response_text:
        return []
    
    lines = [line.strip() for line in response_text.split('\n') if line.strip()]
    filtered_lines = []
    
    for line in lines:
        # ç§»é™¤å›¾ç‰‡æ–‡ä»¶å
        if any(ext in line for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']):
            continue
        # ç§»é™¤æ•°å­—ç¼–å·ï¼ˆå¦‚ "1. "ã€"2. "ç­‰ï¼‰
        elif re.match(r'^\d+\.\s*', line):
            line = re.sub(r'^\d+\.\s*', '', line)
            filtered_lines.append(line)
        # ç§»é™¤å…¶ä»–å¯èƒ½çš„å‰ç¼€
        elif re.match(r'^[â€¢\-*]\s*', line):
            line = re.sub(r'^[â€¢\-*]\s*', '', line)
            filtered_lines.append(line)
        # ç§»é™¤åŒ…å«"Image:"ã€"å›¾ç‰‡:"ç­‰å‰ç¼€çš„è¡Œ
        elif re.match(r'^(Image|å›¾ç‰‡|IMG|img)\s*[:ï¼š]', line, re.IGNORECASE):
            continue
        else:
            filtered_lines.append(line)
    
    # âœ… å…³é”®ï¼šæ¸…æ´—æè¿°æ–‡æœ¬ï¼Œç§»é™¤å›¾ç‰‡åç­‰æ— å…³å†…å®¹
    cleaned_lines = []
    for line in filtered_lines:
        # ç§»é™¤å¯èƒ½åŒ…å«çš„å›¾ç‰‡æ–‡ä»¶å
        line = re.sub(r'\bCOCO_2014_\d{12}\.jpg\b', '', line)
        line = re.sub(r'\b\d{12}\b', '', line)  # ç§»é™¤12ä½æ•°å­—ID
        line = re.sub(r'\bimage\s*\d+\b', '', line, flags=re.IGNORECASE)
        line = line.strip()
        
        # ç§»é™¤å¼€å¤´ç»“å°¾çš„æ ‡ç‚¹
        line = re.sub(r'^[":\-\s]+', '', line)
        line = re.sub(r'[":\-\s]+$', '', line)
        
        if line:
            cleaned_lines.append(line)
    
    return cleaned_lines

async def process_single_image_with_retry(img_name, img_path, semaphore):
    """å¤„ç†å•å¼ å›¾ç‰‡ï¼Œå¦‚æœä¸è¶³5æ¡æè¿°åˆ™é‡æ–°ç”Ÿæˆ"""
    all_descriptions = []
    seen_descriptions = set()
    temperature = 0.8
    regeneration_attempts = 0
    
    while len(all_descriptions) < 5 and regeneration_attempts < MAX_REGENERATION_ATTEMPTS:
        needed = 5 - len(all_descriptions)
        
        if regeneration_attempts > 0:
            await logger.write(f"  â†³ {img_name}: ç¬¬{regeneration_attempts+1}æ¬¡é‡è¯•ï¼Œè¿˜éœ€è¦{needed}æ¡æè¿°\n")
            temperature = min(1.2, temperature + 0.1)
        
        response_text = await generate_single_description(img_path, img_name, temperature, len(all_descriptions), semaphore)
        new_descriptions = parse_single_response(response_text)
        
        if not new_descriptions:
            await logger.write(f"  â†³ {img_name}: ç¬¬{regeneration_attempts+1}æ¬¡è°ƒç”¨è¿”å›ç©ºç»“æœ\n")
            regeneration_attempts += 1
            await asyncio.sleep(2)
            continue
        
        unique_new_descriptions = []
        for desc in new_descriptions:
            # æ¸…æ´—æè¿°æ–‡æœ¬
            cleaned_desc = desc.strip()
            # ç§»é™¤è¿‡çŸ­çš„æè¿°ï¼ˆå°äº3ä¸ªå•è¯ï¼‰
            if len(cleaned_desc.split()) < 3:
                continue
            # æ£€æŸ¥æ˜¯å¦é‡å¤
            if cleaned_desc not in seen_descriptions and cleaned_desc not in all_descriptions:
                unique_new_descriptions.append(cleaned_desc)
                seen_descriptions.add(cleaned_desc)
        
        if unique_new_descriptions:
            all_descriptions.extend(unique_new_descriptions[:needed])
        
        regeneration_attempts += 1
        
        if len(all_descriptions) < 5:
            await asyncio.sleep(1)
    
    if len(all_descriptions) >= 5:
        result = all_descriptions[:5]
        await logger.write(f"âœ“ {img_name}: æˆåŠŸç”Ÿæˆ5æ¡æè¿°ï¼ˆå°è¯•{regeneration_attempts}æ¬¡ï¼‰\n")
        return result, False  # ç¬¬äºŒä¸ªè¿”å›å€¼è¡¨ç¤ºæ˜¯å¦å¤±è´¥
    
    await logger.write(f"âŒ {img_name}: æ— æ³•ç”Ÿæˆ5æ¡æè¿°ï¼Œåªæœ‰{len(all_descriptions)}æ¡\n")
    # âœ… ä¿®æ”¹ï¼šå¤±è´¥æ—¶è¿”å›çº¯å ä½ç¬¦ï¼Œä¸åŒ…å«å›¾ç‰‡å
    return [f"æè¿°ç”Ÿæˆå¤±è´¥_{i+1}" for i in range(5)], True

async def process_single_image(img_name, img_path, semaphore):
    """å¤„ç†å•å¼ å›¾ç‰‡çš„ä¸»å‡½æ•°"""
    try:
        return await process_single_image_with_retry(img_name, img_path, semaphore)
    except Exception as e:
        await logger.write(f"âŒ {img_name}: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)[:50]}\n")
        # âœ… ä¿®æ”¹ï¼šå¼‚å¸¸æ—¶è¿”å›çº¯å ä½ç¬¦
        return [f"å¤„ç†å¼‚å¸¸_{i+1}" for i in range(5)], True

# ==================== æ¯Nå¼ å›¾ç‰‡ä¿å­˜ä¸€ä¸ªæ–‡ä»¶çš„åŠŸèƒ½ ====================
async def save_images_file(file_number, all_descriptions, image_names_to_process, start_idx, end_idx):
    """ä¿å­˜æ¯æ‰¹å›¾ç‰‡çš„æè¿°åˆ°ä¸€ä¸ªæ–‡ä»¶ - åªä¿å­˜çº¯æè¿°æ–‡æœ¬"""
    output_file_path = os.path.join(OUTPUT_DIR, f'train_caps_5_per_image_part{file_number:03d}.txt')
    
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç¬¬ {file_number} ä¸ªæ–‡ä»¶: {output_file_path}")
    print(f"  å›¾ç‰‡ç´¢å¼•èŒƒå›´: {start_idx} åˆ° {end_idx-1}")
    print(f"  åº”åŒ…å«å›¾ç‰‡æ•°: {end_idx - start_idx}")
    
    # ä¿å­˜æ–‡ä»¶
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
        
        # âœ… å…³é”®ä¿®æ”¹ï¼šä¸¥æ ¼æŒ‰åŸå§‹ç´¢å¼•é¡ºåºä¿å­˜ï¼Œåªä¿å­˜çº¯æè¿°æ–‡æœ¬
        lines_to_write = []
        saved_count = 0
        missing_count = 0
        failed_count = 0
        
        print(f"  æ­£åœ¨ä¿å­˜å›¾ç‰‡æè¿°ï¼ˆçº¯æ–‡æœ¬ï¼‰:")
        
        for i in range(start_idx, end_idx):
            img_name = image_names_to_process[i]  # âœ… ä¸¥æ ¼æŒ‰ç…§åŸå§‹é¡ºåº
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æè¿°
            if img_name not in all_descriptions:
                print(f"    âš  å›¾ç‰‡ {i+1}/{end_idx-start_idx}: æ²¡æœ‰æè¿°æ•°æ®")
                # âœ… ä¿®æ”¹ï¼šå ä½ç¬¦ä¸åŒ…å«å›¾ç‰‡å
                descriptions = [f"æè¿°æœªç”Ÿæˆ_{j+1}" for j in range(5)]
                missing_count += 1
            else:
                descriptions = all_descriptions[img_name]
                
                # å¦‚æœæ˜¯ç©ºåˆ—è¡¨
                if not descriptions:
                    print(f"    âš  å›¾ç‰‡ {i+1}/{end_idx-start_idx}: æè¿°åˆ—è¡¨ä¸ºç©º")
                    descriptions = [f"æè¿°ä¸ºç©º_{j+1}" for j in range(5)]
                    missing_count += 1
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤±è´¥æè¿°
            is_failed = any("å¤±è´¥" in desc or "æœªç”Ÿæˆ" in desc or "ä¸ºç©º" in desc or "å¼‚å¸¸" in desc for desc in descriptions)
            if is_failed:
                failed_count += 1
            
            # ç¡®ä¿æœ‰5ä¸ªæè¿°
            if len(descriptions) < 5:
                missing = 5 - len(descriptions)
                placeholders = [f"è¡¥å……æè¿°_{j+1}" for j in range(missing)]
                descriptions = descriptions + placeholders
                missing_count += 1
            elif len(descriptions) > 5:
                descriptions = descriptions[:5]
            
            # âœ… å…³é”®ï¼šåªæ·»åŠ çº¯æè¿°æ–‡æœ¬ï¼Œä¸åŒ…å«ä»»ä½•å›¾ç‰‡åä¿¡æ¯
            for desc in descriptions:
                # ç¡®ä¿æè¿°æ˜¯çº¯æ–‡æœ¬ï¼ˆç§»é™¤ä»»ä½•å¯èƒ½çš„å›¾ç‰‡åæ®‹ç•™ï¼‰
                clean_desc = desc.strip()
                # å†æ¬¡æ£€æŸ¥å¹¶ç§»é™¤å¯èƒ½çš„å›¾ç‰‡å
                clean_desc = re.sub(r'\bCOCO_2014_\d{12}\b', '', clean_desc)
                clean_desc = re.sub(r'\b\d{12}\b', '', clean_desc)
                clean_desc = re.sub(r'\bimage\s*\d+\b', '', clean_desc, flags=re.IGNORECASE)
                clean_desc = clean_desc.strip()
                
                if not clean_desc:
                    clean_desc = "æè¿°å†…å®¹ä¸ºç©º"
                
                lines_to_write.append(clean_desc)
            
            saved_count += 1
            
            # æ¯å¤„ç†10å¼ å›¾ç‰‡è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if (i - start_idx + 1) % 10 == 0:
                print(f"    å·²å¤„ç† {i-start_idx+1}/{end_idx-start_idx} å¼ å›¾ç‰‡")
        
        print(f"\n  å‡†å¤‡å†™å…¥ {len(lines_to_write)} è¡Œçº¯æè¿°æ–‡æœ¬")
        print(f"  æŒ‰é¡ºåºå¤„ç†äº† {saved_count} å¼ å›¾ç‰‡")
        if missing_count > 0:
            print(f"  âš  æœ‰ {missing_count} å¼ å›¾ç‰‡æ²¡æœ‰å®Œæ•´æè¿°æ•°æ®")
        if failed_count > 0:
            print(f"  âŒ æœ‰ {failed_count} å¼ å›¾ç‰‡ç”Ÿæˆå¤±è´¥")
        
        # âœ… éªŒè¯å‰å‡ ä¸ªæè¿°æ˜¯å¦ä¸ºçº¯æ–‡æœ¬
        if lines_to_write:
            print(f"\n  éªŒè¯å‰3ä¸ªæè¿°ï¼ˆåº”ä¸ºçº¯æ–‡æœ¬ï¼‰:")
            for j in range(min(3, len(lines_to_write) // 5)):
                start_line = j * 5
                print(f"    ç¬¬{j+1}å¼ å›¾ç‰‡çš„5æ¡æè¿°:")
                for k in range(5):
                    line_idx = start_line + k
                    if line_idx < len(lines_to_write):
                        desc = lines_to_write[line_idx]
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡å
                        if 'COCO_' in desc or any(str(num) * 3 in desc for num in range(10)):
                            print(f"      âš  æè¿°{k+1}å¯èƒ½åŒ…å«å›¾ç‰‡ä¿¡æ¯: {desc[:50]}...")
                        else:
                            print(f"      âœ“ æè¿°{k+1}: {desc[:50]}...")
        
        # å†™å…¥æ–‡ä»¶ï¼ˆçº¯æè¿°æ–‡æœ¬ï¼Œæ¯è¡Œä¸€æ¡æè¿°ï¼‰
        with open(output_file_path, 'w', encoding='utf-8') as f:
            for line in lines_to_write:
                f.write(line + "\n")
            f.flush()
            os.fsync(f.fileno())
        
        # éªŒè¯æ–‡ä»¶
        if os.path.exists(output_file_path):
            file_size = os.path.getsize(output_file_path)
            print(f"\n  âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ!")
            print(f"    å¤§å°: {file_size} å­—èŠ‚")
            print(f"    å›¾ç‰‡: {saved_count} å¼ ")
            print(f"    æè¿°è¡Œæ•°: {len(lines_to_write)} è¡Œ")
            
            # éªŒè¯æ–‡ä»¶å†…å®¹
            with open(output_file_path, 'r', encoding='utf-8') as f:
                first_few_lines = [f.readline().strip() for _ in range(5)]
                print(f"\n    æ–‡ä»¶å‰5è¡Œå†…å®¹:")
                for idx, line in enumerate(first_few_lines, 1):
                    print(f"      è¡Œ{idx}: {line[:60]}...")
            
            await logger.write(f"\nâœ… å·²ä¿å­˜ç¬¬ {file_number} ä¸ªæ–‡ä»¶: {output_file_path}\n")
            await logger.write(f"   åŒ…å«å›¾ç‰‡ç´¢å¼• {start_idx} åˆ° {end_idx-1} (å…±{saved_count}å¼ å›¾ç‰‡)\n")
            await logger.write(f"   æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚, {len(lines_to_write)} è¡Œæè¿°\n")
            if missing_count > 0:
                await logger.write(f"   âš  {missing_count}å¼ å›¾ç‰‡æ²¡æœ‰å®Œæ•´æè¿°æ•°æ®\n")
            
            return saved_count, failed_count
        else:
            print(f"  âŒ æ–‡ä»¶æœªåˆ›å»º!")
            return 0, 0
            
    except Exception as e:
        print(f"  âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0, 0

# ==================== è¿›åº¦è·Ÿè¸ªå™¨ ====================
class ProgressTracker:
    """è·Ÿè¸ªå¤„ç†è¿›åº¦"""
    def __init__(self, total_images):
        self.total = total_images
        self.processed = 0
        self.lock = Lock()
        self.start_time = time.time()
        self.last_log_time = time.time()
        self.last_progress_log = 0
    
    async def update(self, count=1):
        async with self.lock:
            self.processed += count
            
            # ç¡®ä¿ä¸ä¼šè¶…è¿‡æ€»æ•°
            if self.processed > self.total:
                self.processed = self.total
            
            progress = self.processed / self.total * 100 if self.total > 0 else 0
            elapsed = time.time() - self.start_time
            
            # æ¯10å¼ å›¾ç‰‡æˆ–æ¯5ç§’è®°å½•ä¸€æ¬¡è¿›åº¦
            current_time = time.time()
            if self.processed - self.last_progress_log >= 10 or current_time - self.last_log_time > 5:
                if elapsed > 0:
                    speed = self.processed / elapsed
                    # ä¿®å¤ETAè®¡ç®—ï¼Œé¿å…è´Ÿæ•°
                    eta = (self.total - self.processed) / speed if speed > 0 and self.processed < self.total else 0
                    
                    await logger.write(
                        f"ğŸ“Š è¿›åº¦: {self.processed}/{self.total} ({progress:.1f}%) | "
                        f"é€Ÿåº¦: {speed:.2f} å¼ /ç§’ | ETA: {eta/60:.1f} åˆ†é’Ÿ\n"
                    )
                self.last_log_time = current_time
                self.last_progress_log = self.processed

# ==================== å¤„ç†å•ä¸ªæ–‡ä»¶çš„å›¾ç‰‡ ====================
async def process_single_file_images(file_number, start_idx, end_idx, image_names_to_process, 
                                    all_descriptions, failed_images, semaphore, progress_tracker):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„æ‰€æœ‰å›¾ç‰‡"""
    print(f"\nğŸ“ å¼€å§‹å¤„ç†ç¬¬ {file_number} ä¸ªæ–‡ä»¶")
    print(f"  å›¾ç‰‡ç´¢å¼•: {start_idx} åˆ° {end_idx-1}")
    print(f"  å…± {end_idx - start_idx} å¼ å›¾ç‰‡")
    
    file_successful_count = 0
    file_failed_count = 0
    
    # è·å–å½“å‰æ–‡ä»¶çš„æ‰€æœ‰å›¾ç‰‡
    file_image_names = image_names_to_process[start_idx:end_idx]
    print(f"  å‰5å¼ å›¾ç‰‡: {file_image_names[:5]}")
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = []
    for idx in range(start_idx, end_idx):
        img_name = image_names_to_process[idx]
        img_path = os.path.join(IMAGE_DIR, img_name)
        
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(img_path):
            await logger.write(f"âŒ {img_name}: æ–‡ä»¶ä¸å­˜åœ¨\n")
            async with failed_images_lock:
                # âœ… ä¿®æ”¹ï¼šå¤±è´¥æè¿°ä¸åŒ…å«å›¾ç‰‡å
                all_descriptions[img_name] = [f"æ–‡ä»¶ç¼ºå¤±_{j+1}" for j in range(5)]
                failed_images.append(img_name)
                file_failed_count += 1
            await progress_tracker.update(1)
            continue
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
        if img_name in all_descriptions and all_descriptions[img_name]:
            await logger.write(f"â© {img_name}: å·²å¤„ç†ï¼Œè·³è¿‡\n")
            await progress_tracker.update(1)
            continue
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        task = asyncio.create_task(process_single_image(img_name, img_path, semaphore))
        tasks.append((task, idx, img_name))
    
    if not tasks:
        print(f"  â© å½“å‰æ–‡ä»¶æ²¡æœ‰éœ€è¦å¤„ç†çš„ä»»åŠ¡")
        return file_successful_count, file_failed_count
    
    print(f"  ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡ (å¹¶å‘æ•°: {MAX_CONCURRENT_REQUESTS})...")
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    results = await asyncio.gather(*[t[0] for t in tasks], return_exceptions=True)
    
    # å¤„ç†ç»“æœ
    for (task, idx, img_name), result in zip(tasks, results):
        if isinstance(result, Exception):
            await logger.write(f"âŒ {img_name}: ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {str(result)[:50]}\n")
            async with checkpoint_lock:
                # âœ… ä¿®æ”¹ï¼šå¼‚å¸¸æè¿°ä¸åŒ…å«å›¾ç‰‡å
                all_descriptions[img_name] = [f"ä»»åŠ¡å¤±è´¥_{i+1}" for i in range(5)]
            async with failed_images_lock:
                failed_images.append(img_name)
            file_failed_count += 1
            await progress_tracker.update(1)
        elif isinstance(result, tuple) and len(result) == 2:
            # æ­£å¸¸è¿”å› (descriptions, is_failed)
            descriptions, is_failed = result
            async with checkpoint_lock:
                all_descriptions[img_name] = descriptions
            if is_failed:
                async with failed_images_lock:
                    failed_images.append(img_name)
                file_failed_count += 1
            else:
                file_successful_count += 1
            await progress_tracker.update(1)
        else:
            await logger.write(f"âš  {img_name}: è¿”å›ç»“æœæ ¼å¼å¼‚å¸¸\n")
            async with checkpoint_lock:
                # âœ… ä¿®æ”¹ï¼šå¼‚å¸¸æè¿°ä¸åŒ…å«å›¾ç‰‡å
                all_descriptions[img_name] = [f"æ ¼å¼é”™è¯¯_{i+1}" for i in range(5)]
            async with failed_images_lock:
                failed_images.append(img_name)
            file_failed_count += 1
            await progress_tracker.update(1)
    
    return file_successful_count, file_failed_count

# ==================== ä¸»ç¨‹åº - æŒ‰æ–‡ä»¶é¡ºåºå¤„ç† ====================
async def main_async():
    """ä¸»å¼‚æ­¥å‡½æ•° - æŒ‰æ–‡ä»¶é¡ºåºå¤„ç†"""
    global logger, checkpoint_lock, failed_images_lock
    
    # åˆå§‹åŒ–å…¨å±€é”
    checkpoint_lock = Lock()
    failed_images_lock = Lock()
    
    logger = AsyncLogger(LOG_FILE)
    sys.stdout = sys.stderr = logger.terminal
    
    try:
        print("=" * 60)
        print("ğŸš€ å›¾ç‰‡æè¿°ç”Ÿæˆç¨‹åºå¯åŠ¨")
        if TEST_MODE:
            print("ğŸ”¬ æµ‹è¯•æ¨¡å¼: 2å¹¶å‘ï¼Œæ¯10å¼ å›¾ç‰‡ä¿å­˜ä¸€ä¸ªæ–‡ä»¶")
        print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        print(f"å¹¶å‘æ•°: {MAX_CONCURRENT_REQUESTS}")
        print(f"ä¿å­˜é—´éš”: æ¯{SAVE_INTERVAL}å¼ å›¾ç‰‡ä¿å­˜ä¸€ä¸ªæ–‡ä»¶")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®ï¼ˆä¿è¯é¡ºåºï¼‰
        print("\nğŸ“‹ æ­£åœ¨åŠ è½½å›¾ç‰‡æ•°æ®...")
        image_names_to_process = await load_data()
        total_images = len(image_names_to_process)
        
        if total_images == 0:
            print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„å›¾ç‰‡!")
            return
        
        print(f"âœ… æ€»å…±éœ€è¦å¤„ç† {total_images} å¼ å›¾ç‰‡")
        total_files = (total_images + SAVE_INTERVAL - 1) // SAVE_INTERVAL
        print(f"   å°†ä¿å­˜ {total_files} ä¸ªæ–‡ä»¶")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        print("\nğŸ” æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹...")
        checkpoint = await load_checkpoint()
        processed_count = checkpoint.get("processed_count", 0)
        file_count = checkpoint.get("file_count", 0)
        failed_images = checkpoint.get("failed_images", [])
        all_descriptions = checkpoint.get("all_descriptions", {})
        current_file_number = checkpoint.get("current_file_number", 1)
        current_file_start_idx = checkpoint.get("current_file_start_idx", 0)
        
        # åˆå§‹åŒ–æè¿°å­—å…¸
        for img_name in image_names_to_process:
            if img_name not in all_descriptions:
                all_descriptions[img_name] = []
        
        print(f"ğŸ“Š æ£€æŸ¥ç‚¹æ¢å¤:")
        print(f"  å·²å¤„ç†å›¾ç‰‡: {processed_count}/{total_images}")
        print(f"  å·²ä¿å­˜æ–‡ä»¶: {file_count}/{total_files}")
        print(f"  å½“å‰å¤„ç†æ–‡ä»¶: ç¬¬{current_file_number}ä¸ª")
        print(f"  å¤±è´¥å›¾ç‰‡æ•°: {len(failed_images)}")
        print(f"  å¹¶å‘æ•°: {MAX_CONCURRENT_REQUESTS}")
        
        await logger.write(f"ğŸ” æ£€æŸ¥ç‚¹æ¢å¤: {processed_count}/{total_images} å¼ å›¾ç‰‡å·²å¤„ç†\n")
        await logger.write(f"âŒ å¤±è´¥å›¾ç‰‡æ•°: {len(failed_images)}\n")
        await logger.write(f"{'='*60}\n")
        
        start_time = time.time()
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)
        progress_tracker = ProgressTracker(total_images)
        
        # è®¾ç½®è¿›åº¦ä»å·²å¤„ç†æ•°é‡å¼€å§‹
        progress_tracker.processed = processed_count
        
        # âœ… å…³é”®ä¿®æ”¹ï¼šæŒ‰æ–‡ä»¶é¡ºåºå¤„ç†ï¼Œå¤„ç†å®Œä¸€ä¸ªæ–‡ä»¶å°±ä¿å­˜
        total_successful = 0
        total_failed = 0
        
        for file_number in range(current_file_number, total_files + 1):
            # è®¡ç®—å½“å‰æ–‡ä»¶çš„å›¾ç‰‡èŒƒå›´
            start_idx = (file_number - 1) * SAVE_INTERVAL
            end_idx = min(start_idx + SAVE_INTERVAL, total_images)
            
            print(f"\n{'='*60}")
            print(f"ğŸ“ å¤„ç†ç¬¬ {file_number}/{total_files} ä¸ªæ–‡ä»¶")
            print(f"  å›¾ç‰‡ç´¢å¼•: {start_idx} åˆ° {end_idx-1}")
            print(f"  å…± {end_idx - start_idx} å¼ å›¾ç‰‡")
            print(f"{'='*60}")
            
            await logger.write(f"\nğŸ“ å¼€å§‹å¤„ç†ç¬¬ {file_number} ä¸ªæ–‡ä»¶ (å›¾ç‰‡ {start_idx}-{end_idx-1})\n")
            
            # å¤„ç†å½“å‰æ–‡ä»¶çš„æ‰€æœ‰å›¾ç‰‡
            file_successful, file_failed = await process_single_file_images(
                file_number, start_idx, end_idx, image_names_to_process,
                all_descriptions, failed_images, semaphore, progress_tracker
            )
            
            total_successful += file_successful
            total_failed += file_failed
            
            print(f"\nğŸ“Š ç¬¬ {file_number} ä¸ªæ–‡ä»¶å¤„ç†å®Œæˆ:")
            print(f"  æˆåŠŸ: {file_successful} å¼ ")
            print(f"  å¤±è´¥: {file_failed} å¼ ")
            
            # âœ… å…³é”®ï¼šç«‹å³ä¿å­˜å½“å‰æ–‡ä»¶ï¼ˆåªä¿å­˜çº¯æè¿°æ–‡æœ¬ï¼‰
            print(f"\nğŸ’¾ ç«‹å³ä¿å­˜ç¬¬ {file_number} ä¸ªæ–‡ä»¶ï¼ˆçº¯æè¿°æ–‡æœ¬ï¼‰...")
            saved_count, failed_in_file = await save_images_file(
                file_number, all_descriptions, image_names_to_process, start_idx, end_idx
            )
            
            if saved_count > 0:
                print(f"  âœ… ç¬¬ {file_number} ä¸ªæ–‡ä»¶ä¿å­˜æˆåŠŸ!")
                print(f"     åŒ…å« {saved_count} å¼ å›¾ç‰‡çš„æè¿°")
                
                # æ›´æ–°æ£€æŸ¥ç‚¹
                checkpoint_data = {
                    "processed_count": end_idx,  # å·²å¤„ç†åˆ°å“ªä¸ªç´¢å¼•
                    "file_count": file_number,    # å·²ä¿å­˜çš„æ–‡ä»¶æ•°
                    "failed_images": failed_images,
                    "all_descriptions": all_descriptions,
                    "current_file_number": file_number + 1,  # ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„æ–‡ä»¶
                    "current_file_start_idx": end_idx,      # ä¸‹ä¸€ä¸ªæ–‡ä»¶çš„å¼€å§‹ç´¢å¼•
                    "timestamp": time.time()
                }
                
                async with checkpoint_lock:
                    await save_checkpoint(checkpoint_data)
                
                print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹å·²æ›´æ–°: æ–‡ä»¶{file_number}, å›¾ç‰‡{end_idx}")
            else:
                print(f"  âŒ ç¬¬ {file_number} ä¸ªæ–‡ä»¶ä¿å­˜å¤±è´¥!")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            current_time = time.time()
            elapsed = current_time - start_time
            if elapsed > 0:
                current_processed = file_number * SAVE_INTERVAL if file_number < total_files else total_images
                speed = current_processed / elapsed
                remaining_files = total_files - file_number
                eta = (remaining_files * SAVE_INTERVAL) / speed if speed > 0 else 0
                
                print(f"\nğŸ“ˆ æ€»ä½“è¿›åº¦:")
                print(f"  å·²å¤„ç†æ–‡ä»¶: {file_number}/{total_files}")
                print(f"  å·²å¤„ç†å›¾ç‰‡: {current_processed}/{total_images}")
                print(f"  å¹³å‡é€Ÿåº¦: {speed:.2f} å¼ /ç§’")
                if remaining_files > 0:
                    print(f"  é¢„è®¡å‰©ä½™æ—¶é—´: {eta/60:.1f} åˆ†é’Ÿ")
            
            # æ¯ä¸ªæ–‡ä»¶å¤„ç†åç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…APIé™åˆ¶
            if file_number < total_files:
                print(f"\nâ³ å‡†å¤‡å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶ï¼Œç­‰å¾…3ç§’...")
                await asyncio.sleep(3)
        
        # æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ
        print(f"\n{'='*60}")
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ!")
        
        # ä¿å­˜å¤±è´¥å›¾ç‰‡åˆ—è¡¨
        if failed_images:
            failed_file = os.path.join(OUTPUT_DIR, 'failed_train_images.txt')
            print(f"\nğŸ“‹ ä¿å­˜å¤±è´¥å›¾ç‰‡åˆ—è¡¨åˆ°: {failed_file}")
            
            try:
                with open(failed_file, 'w', encoding='utf-8') as f:
                    for img_name in failed_images:
                        f.write(f"{img_name}\n")
                print(f"å¤±è´¥å›¾ç‰‡åˆ—è¡¨å·²ä¿å­˜ ({len(failed_images)}å¼ )")
            except Exception as e:
                print(f"ä¿å­˜å¤±è´¥å›¾ç‰‡åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        
        total_time = time.time() - start_time
        
        print(f"\n{'='*60}")
        print("âœ… å¤„ç†å®Œæˆ!")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {total_images/total_time:.2f} å¼ /ç§’")
        print(f"ğŸ“Š æˆåŠŸ: {total_successful} å¼  | å¤±è´¥: {total_failed} å¼ ")
        print(f"ğŸ“ ä¿å­˜æ–‡ä»¶æ•°: {total_files} ä¸ª")
        print(f"ğŸ“ æ€»æè¿°è¡Œæ•°: {total_images * 5} è¡Œ")
        print(f"{'='*60}")
        
        # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
        print("\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ï¼ˆçº¯æè¿°æ–‡æœ¬ï¼‰:")
        import glob
        files = glob.glob(os.path.join(OUTPUT_DIR, "train_caps_5_per_image_part*.txt"))
        for file in sorted(files):
            size = os.path.getsize(file)
            with open(file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            print(f"  {os.path.basename(file)} - {size} å­—èŠ‚, {len(lines)} è¡Œ, {len(lines)//5} å¼ å›¾ç‰‡")
            
            # æ˜¾ç¤ºæ–‡ä»¶å‰å‡ è¡Œå†…å®¹
            if file == files[0]:  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å†…å®¹
                print(f"    å‰3è¡Œå†…å®¹:")
                for i, line in enumerate(lines[:3], 1):
                    print(f"      è¡Œ{i}: {line.strip()[:60]}...")
        
        # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆå¤„ç†å®Œæˆï¼‰
        if os.path.exists(CHECKPOINT_FILE):
            os.remove(CHECKPOINT_FILE)
            print("\nğŸ§¹ æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶")
        
        await logger.write(f"\n{'='*60}\n")
        await logger.write("âœ… å¤„ç†å®Œæˆ!\n")
        await logger.write(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’\n")
        await logger.write(f"ğŸš€ å¹³å‡é€Ÿåº¦: {total_images/total_time:.2f} å¼ /ç§’\n")
        await logger.write(f"ğŸ“Š æˆåŠŸ: {total_successful} å¼  | å¤±è´¥: {total_failed} å¼ \n")
        await logger.write(f"ğŸ“ ä¿å­˜æ–‡ä»¶æ•°: {total_files} ä¸ª\n")
        await logger.write(f"ğŸ“ æ€»æè¿°è¡Œæ•°: {total_images * 5} è¡Œ\n")
        await logger.write(f"{'='*60}\n")
        
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc(file=logger.terminal)
        
        # å‘ç”Ÿå¼‚å¸¸æ—¶ä¿å­˜å½“å‰çŠ¶æ€
        try:
            checkpoint_data = {
                "processed_count": progress_tracker.processed if 'progress_tracker' in locals() else 0,
                "file_count": file_count if 'file_count' in locals() else 0,
                "failed_images": failed_images if 'failed_images' in locals() else [],
                "all_descriptions": all_descriptions if 'all_descriptions' in locals() else {},
                "current_file_number": current_file_number if 'current_file_number' in locals() else 1,
                "current_file_start_idx": current_file_start_idx if 'current_file_start_idx' in locals() else 0,
                "timestamp": time.time()
            }
            
            async with checkpoint_lock:
                await save_checkpoint(checkpoint_data)
            print("ğŸ’¾ å·²ä¿å­˜å¼‚å¸¸æ—¶çš„æ£€æŸ¥ç‚¹")
        except:
            pass
    finally:
        # ç­‰å¾…æ‰€æœ‰I/Oæ“ä½œå®Œæˆ
        await asyncio.sleep(1)
        await logger.close()

# ==================== å…¥å£ ====================
if __name__ == "__main__":
    print("ç¨‹åºå¯åŠ¨...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    # æ£€æŸ¥æ˜¯å¦é‡å¤è¿è¡Œ
    if os.path.exists(PID_FILE):
        with open(PID_FILE, 'r') as f:
            old_pid = f.read().strip()
        try:
            os.kill(int(old_pid), 0)
            print(f"è­¦å‘Š: è¿›ç¨‹ {old_pid} å·²åœ¨è¿è¡Œ!")
            print(f"å¦‚éœ€é‡å¯ï¼Œè¯·åˆ é™¤: {PID_FILE}")
            sys.exit(1)
        except OSError:
            os.remove(PID_FILE)
    
    # ä¿å­˜PID
    with open(PID_FILE, 'w') as f:
        f.write(str(os.getpid()))
    print(f"PIDæ–‡ä»¶: {PID_FILE}")
    
    # è®¾ç½®ä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        print("å¼€å§‹å¼‚æ­¥ä¸»ç¨‹åº...")
        asyncio.run(main_async())
    except Exception as e:
        print(f"ä¸»ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if os.path.exists(PID_FILE):
            os.remove(PID_FILE)
        print("ç¨‹åºç»“æŸ")